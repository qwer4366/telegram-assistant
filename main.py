# main.py
import logging
import io
import qrcode
import asyncio
import os
from dotenv import load_dotenv # <-- Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ù…Ù† .env
import pandas as pd # <-- Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ pandas

# ØªØ­Ù…ÙŠÙ„ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ù…Ù† Ù…Ù„Ù .env
load_dotenv()

import json # For JSON processing
import csv # For CSV schema extraction
import re # For parsing --file in /askdata
from rapidfuzz import fuzz, process as rapidfuzz_process # For fuzzy file name matching

from openai import OpenAI
# import google.generativeai as genai # Ù…Ø¹Ø·Ù„ Ù…Ø¤Ù‚ØªÙ‹Ø§

import fitz  # PyMuPDF
import pytesseract
from io import BytesIO # Already have import io, but BytesIO is useful directly
from PIL import Image

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, ReplyKeyboardMarkup, KeyboardButton
from telegram.constants import ChatAction
from telegram.ext import (
    Application,
    ApplicationBuilder,
    ContextTypes,
    CommandHandler,
    MessageHandler,
    filters,
    CallbackQueryHandler
)

# Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Google Drive API
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaIoBaseUpload, MediaIoBaseDownloader

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---
BOT_TOKEN = os.getenv("BOT_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DRIVE_BOT_UPLOAD_FOLDER_ID = os.getenv("DRIVE_BOT_UPLOAD_FOLDER_ID")
if not DRIVE_BOT_UPLOAD_FOLDER_ID:
    logger.warning("!!! DRIVE_BOT_UPLOAD_FOLDER_ID is not set in .env. File uploads to Drive will fail. Please create a folder in Google Drive and set its ID as DRIVE_BOT_UPLOAD_FOLDER_ID in the .env file.")

ADMIN_ID_1 = 1263152179
ADMIN_ID_2 = 697852646
ADMIN_IDS = [ADMIN_ID_1, ADMIN_ID_2]

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Google Drive API ---
# Full access scope for Google Drive (read, write, create, delete).
# If you previously authenticated with read-only, you might need to delete token.json and re-authenticate.

# --- Conversation Handler States for /searchdata ---
SELECT_FILE, SELECT_COLUMN, SELECT_MATCH_TYPE, INPUT_SEARCH_VALUE = range(4)
SCOPES = ['https://www.googleapis.com/auth/drive']
CLIENT_SECRET_FILE = 'client_secret.json'
TOKEN_FILE = 'token.json'

logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO # ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯ØªÙ‡ Ø¥Ù„Ù‰ INFO Ø§Ù„Ø¢Ù† Ø£Ùˆ ØªØ±ÙƒÙ‡ DEBUG Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„
)
logger = logging.getLogger(__name__)
# logging.getLogger("httpx").setLevel(logging.WARNING) # ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© ØªÙØ¹ÙŠÙ„ Ù‡Ø°Ø§ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø¶ÙˆØ¶Ø§Ø¡ httpx
# logging.getLogger("telegram.ext").setLevel(logging.DEBUG) # ÙŠÙ…ÙƒÙ†Ùƒ ØªØ±Ùƒ Ù‡Ø°Ø§ DEBUG Ø£Ùˆ Ø¥Ø¹Ø§Ø¯ØªÙ‡ Ù„Ù€ INFO


# --- Funciones de extracciÃ³n de texto PDF ---
def sync_extract_text_from_pdf(pdf_bytes: bytes) -> tuple[int, str]:
    text = ""
    num_pages = 0
    doc = None  # Initialize doc to None for the finally block
    pdf_stream = BytesIO(pdf_bytes) # Keep stream open for potential OCR pass

    try:
        doc = fitz.open(stream=pdf_stream, filetype="pdf")
        num_pages = doc.page_count
        for page_num in range(num_pages):
            page = doc.load_page(page_num)
            text += page.get_text("text") # Specify text format

        if num_pages > 0 and (not text.strip() or len(text.strip()) < 50): # Condition for OCR
            logger.info(f"Initial text extraction short or empty (length {len(text.strip())}, pages {num_pages}). Attempting OCR.")
            ocr_full_text = ""
            # Re-open doc if it was closed or use a fresh stream; for now, assume doc is fine.
            # If doc was closed, we'd need: pdf_stream.seek(0); doc = fitz.open(stream=pdf_stream, filetype="pdf")
            # However, fitz.open already read from the stream, so for OCR, we might need to reopen from original bytes if stream is consumed.
            # For simplicity, let's assume the 'doc' object can still be used if it wasn't closed.
            # If PyMuPDF consumes the stream, it's safer to re-create BytesIO for each pass or keep original bytes.
            # For now, we'll assume 'doc' is reusable for page loading if not closed.

            # Re-iterate for OCR if doc is still valid for page loading
            # If `doc.load_page` fails after initial text extraction, `doc` would need to be reopened.
            # This example assumes `doc` can still be used. A more robust solution might re-open from `pdf_bytes`.

            # Ensure doc is re-opened if it was closed or stream was fully consumed.
            # For this implementation, we'll assume `doc` is from the initial try block and still usable.
            # If `page.get_text()` fully consumes parts of the stream needed by `get_pixmap`,
            # then `doc` should be reopened using a fresh BytesIO stream from `pdf_bytes`.
            # Let's try to use the existing 'doc' and pages.

            # If initial text extraction already iterated through pages, the `doc` object is fine.

            for page_num in range(num_pages):
                page = doc.load_page(page_num)
                try:
                    pix = page.get_pixmap()
                    img_bytes = pix.tobytes("png")
                    img = Image.open(BytesIO(img_bytes))
                    # Specify languages: Arabic ('ara') and English ('eng')
                    ocr_text_page = pytesseract.image_to_string(img, lang='ara+eng')
                    ocr_full_text += ocr_text_page + "\n" # Add newline between pages
                except pytesseract.TesseractNotFoundError:
                    logger.error("Tesseract is not installed or not found in PATH.")
                    # Return original short text or error if Tesseract is missing
                    return num_pages, text.strip() if text.strip() else "OCR Error: Tesseract not found."
                except Exception as ocr_exc:
                    logger.error(f"Error during OCR for page {page_num + 1}: {ocr_exc}")
                    # Continue to next page or return what we have / original text

            if ocr_full_text.strip():
                logger.info("OCR process yielded text. Using OCR text.")
                text = ocr_full_text
            else:
                logger.info("OCR process did not yield any new text.")

        return num_pages, text.strip()

    except fitz.errors.FitzError as e:
        logger.error(f"PyMuPDF (Fitz) error processing PDF: {e}")
        # Consider if OCR should be attempted even if PyMuPDF fails initially.
        # For now, returning error. A more advanced flow could try OCR directly on images if fitz fails.
        return 0, "Error processing PDF with PyMuPDF."
    except Exception as e:
        logger.error(f"General error in sync_extract_text_from_pdf: {e}")
        return 0, f"General error processing PDF: {type(e).__name__}"
    finally:
        if doc:
            doc.close()
        # pdf_stream.close() # Close the BytesIO stream

async def extract_text_from_pdf(pdf_bytes: bytes) -> tuple[int, str]:
    # Runs the synchronous PDF extraction in a separate thread
    return await asyncio.to_thread(sync_extract_text_from_pdf, pdf_bytes)


if not BOT_TOKEN:
    logger.critical("CRITICAL: BOT_TOKEN is not set. The bot cannot start.")
    exit()
if not OPENAI_API_KEY:
    logger.warning("!!! OPENAI_API_KEY not set. OpenAI features will not work.")

# --- Ø¯ÙˆØ§Ù„ Google Drive ---
async def get_gdrive_service_async():
    def _authenticate_gdrive():
        creds = None
        if os.path.exists(TOKEN_FILE):
            try:
                creds = Credentials.from_authorized_user_file(TOKEN_FILE, SCOPES)
            except Exception as e:
                logger.error(f"Error loading token file '{TOKEN_FILE}': {e}.")
                creds = None
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                try:
                    logger.info("Attempting to refresh Google Drive token...")
                    creds.refresh(Request())
                    logger.info("Google Drive token refreshed successfully.")
                except Exception as e:
                    logger.error(f"Failed to refresh Google Drive token: {e}.")
                    creds = None
            if not creds:
                try:
                    logger.info(f"'{TOKEN_FILE}' not found or invalid. Starting auth flow.")
                    if not os.path.exists(CLIENT_SECRET_FILE):
                        logger.error(f"Critical: '{CLIENT_SECRET_FILE}' not found.")
                        return None
                    flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)
                    creds = flow.run_local_server(port=0) 
                    logger.info("Google Drive authentication flow completed.")
                except Exception as e:
                    logger.error(f"Error in Google Drive authentication flow: {e}")
                    return None
            try:
                with open(TOKEN_FILE, 'w') as token_file: token_file.write(creds.to_json())
                logger.info(f"Google Drive token saved to {TOKEN_FILE}")
            except Exception as e: logger.error(f"Failed to save Google Drive token: {e}")
        try:
            service = build('drive', 'v3', credentials=creds)
            logger.info("Google Drive service object created.")
            return service
        except Exception as e:
            logger.error(f'Error building Google Drive service: {e}')
            if hasattr(e, 'resp') and e.resp.status in [401, 403]:
                if os.path.exists(TOKEN_FILE): os.remove(TOKEN_FILE)
            return None
    return await asyncio.to_thread(_authenticate_gdrive)

# --- Funciones de subida a Google Drive ---
def upload_to_gdrive_sync(gdrive_service, file_bytes: bytes, file_name: str, mime_type: str, folder_id: str) -> str | None:
    try:
        media_body = MediaIoBaseUpload(io.BytesIO(file_bytes), mimetype=mime_type, resumable=True)
        file_metadata = {'name': file_name}
        if folder_id and folder_id.strip() != "": # Ensure folder_id is not empty or just whitespace
            file_metadata['parents'] = [folder_id]

        created_file = gdrive_service.files().create(
            body=file_metadata,
            media_body=media_body,
            fields='id, webViewLink'
        ).execute()

        file_link = created_file.get('webViewLink')
        logger.info(f"Uploaded file '{file_name}' to Drive. ID: {created_file.get('id')}, Link: {file_link}")
        return file_link
    except HttpError as error:
        logger.error(f"HttpError during GDrive upload for '{file_name}': {error}")
        return None
    except Exception as e:
        logger.error(f"General error during GDrive upload for '{file_name}': {e}")
        return None

async def upload_to_gdrive_async(gdrive_service, file_bytes: bytes, file_name: str, mime_type: str, folder_id: str) -> str | None:
    return await asyncio.to_thread(upload_to_gdrive_sync, gdrive_service, file_bytes, file_name, mime_type, folder_id)

# --- OpenAI Helper Function (Modified) ---
async def get_openai_response(api_key: str, messages: list) -> str:
    try:
        logger.info(f"Sending request to OpenAI API with messages: {messages}")
        def generate_sync():
            client = OpenAI(api_key=api_key)
            completion = client.chat.completions.create(
                model="gpt-4o-mini", # Or your preferred model
                messages=messages,
                max_tokens=1000 # Increased max_tokens for potentially more detailed data explanations
            )
            return completion.choices[0].message.content.strip()
        assistant_reply = await asyncio.to_thread(generate_sync)
        logger.info("Received response from OpenAI API.")
        return assistant_reply if assistant_reply else "Ù„Ù… Ø£ØªÙ„Ù‚ Ø±Ø¯Ù‹Ø§ Ù†ØµÙŠÙ‹Ø§ Ù…Ù† OpenAI."
    except Exception as e:
        logger.error(f"Error communicating with OpenAI API: {e}")
        error_message = str(e).lower()
        if "invalid api key" in error_message or "incorrect api key" in error_message:
             return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù…ÙØªØ§Ø­ OpenAI API ØºÙŠØ± ØµØ§Ù„Ø­. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡."
        if "quota" in error_message or "rate limit" in error_message:
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù‚Ø¯ ØªØ¬Ø§ÙˆØ²Øª Ø­ØµØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ùˆ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·Ù„Ø¨Ø§Øª Ù„Ù€ OpenAI API. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ù‹Ø§."
        return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ OpenAI: ({type(e).__name__})"

# --- Funciones para BÃºsqueda de Archivos y Schema en Google Drive ---
def find_file_in_gdrive_sync(gdrive_service, filename_query: str) -> tuple[str | None, str | None, str | None]:
    try:
        # Fetch a broader set of files to perform fuzzy matching on
        # For now, fetching from root, ordered by most recent. This can be refined.
        # Consider targetting specific folders if context allows, or increasing pageSize if necessary.
        drive_query = "trashed = false and mimeType != 'application/vnd.google-apps.folder'" # Exclude folders from fuzzy search pool for now
        logger.info(f"Fetching up to 100 files from GDrive for fuzzy matching against: '{filename_query}'")
        results = gdrive_service.files().list(
            q=drive_query,
            pageSize=100,  # Fetch a decent pool of candidates
            orderBy="modifiedTime desc",
            fields="files(id, name, mimeType)"
        ).execute()

        items = results.get('files', [])
        if not items:
            logger.info("No files found in GDrive to perform fuzzy matching.")
            return None, None, None

        # Prepare choices for rapidfuzz: a dictionary mapping file ID to file name
        choices = {item['id']: item['name'] for item in items}

        # Use rapidfuzz to find the best match above a certain score cutoff
        # WRatio is good for finding matches even when words are out of order or strings are of different lengths
        best_match = rapidfuzz_process.extractOne(filename_query, choices, scorer=fuzz.WRatio, score_cutoff=70) # score_cutoff (0-100)

        if best_match:
            # best_match is a tuple: (matched_name_from_choices, score, original_key_from_choices (which is file_id))
            matched_name_str, score, file_id_str = best_match
            logger.info(f"Fuzzy match for '{filename_query}': Found '{matched_name_str}' (ID: {file_id_str}) with score {score:.2f}%.")

            # Retrieve the original item to get its mimeType
            original_item = next((item for item in items if item['id'] == file_id_str), None)
            if original_item:
                return original_item.get('id'), original_item.get('name'), original_item.get('mimeType')
            else:
                # This should ideally not happen if file_id_str came from our choices keys
                logger.error(f"Consistency error: Could not find original item for ID {file_id_str} after fuzzy matching.")
                return None, None, None
        else:
            logger.info(f"No suitable fuzzy match found for '{filename_query}' with cutoff 70%.")
            return None, None, None

    except HttpError as error:
        logger.error(f"HttpError during GDrive file list for fuzzy matching '{filename_query}': {error}")
        return None, None, None
    except Exception as e:
        logger.error(f"General error during GDrive file list for fuzzy matching '{filename_query}': {e}", exc_info=True)
        return None, None, None

async def find_file_in_gdrive_async(gdrive_service, filename_query: str) -> tuple[str | None, str | None, str | None]:
    return await asyncio.to_thread(find_file_in_gdrive_sync, gdrive_service, filename_query)

def get_schema_from_file_sync(gdrive_service, file_id: str, mime_type: str) -> list[str] | None: # Return type updated
    try:
        logger.info(f"Attempting to download file ID {file_id} for schema extraction (MIME: {mime_type}).")
        request = gdrive_service.files().get_media(fileId=file_id)
        file_content_stream = io.BytesIO()
        downloader = MediaIoBaseDownloader(file_content_stream, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
            # logger.debug(f"Schema download progress: {int(status.progress() * 100)}%.") # Can be verbose

        file_bytes = file_content_stream.getvalue()
        logger.info(f"File ID {file_id} downloaded successfully for schema extraction.")

        column_names = []
        if mime_type == 'text/csv':
            try:
                # Try decoding with utf-8 first, then with common alternatives if it fails
                text_content = None
                common_encodings = ['utf-8', 'iso-8859-1', 'windows-1252', 'windows-1256'] # Added windows-1256 for Arabic
                for encoding in common_encodings:
                    try:
                        text_content = file_bytes.decode(encoding)
                        logger.info(f"Decoded CSV {file_id} with {encoding}.")
                        break
                    except UnicodeDecodeError:
                        logger.debug(f"Failed to decode CSV {file_id} with {encoding}.")

                if text_content is None:
                    logger.error(f"Could not decode CSV {file_id} with any common encoding.")
                    return "ØªØ¹Ø°Ø± ÙÙƒ ØªØ±Ù…ÙŠØ² Ù…Ù„Ù CSV Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ±Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©."

                csv_file = io.StringIO(text_content)
                reader = csv.reader(csv_file)
                column_names = next(reader) # Get header row
            except StopIteration: # Empty file
                logger.warning(f"CSV file {file_id} is empty or has no header.")
                return "Ù…Ù„Ù CSV ÙØ§Ø±Øº Ø£Ùˆ Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ø¤ÙˆØ³ Ø£Ø¹Ù…Ø¯Ø©."
            except csv.Error as e_csv:
                logger.error(f"CSV processing error for {file_id}: {e_csv}")
                return f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù CSV: {e_csv}"
            except UnicodeDecodeError as e_unicode: # Fallback, though loop above should handle
                logger.error(f"Final UnicodeDecodeError for CSV {file_id}: {e_unicode}")
                return "Ø®Ø·Ø£ ÙÙŠ ÙÙƒ ØªØ±Ù…ÙŠØ² Ù…Ù„Ù CSV. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù Ø¨ØªØ±Ù…ÙŠØ² ØµØ­ÙŠØ­ (Ù…Ø«Ù„ UTF-8)."

        elif mime_type in ['application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']:
            try:
                # Reading only the header row by using nrows=0 and then getting columns
                # Forcing openpyxl for xlsx if available, as xlrd might be deprecated/removed for xlsx
                engine = 'openpyxl' if mime_type == 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' else None
                df = pd.read_excel(io.BytesIO(file_bytes), sheet_name=0, nrows=0, engine=engine)
                column_names = df.columns.tolist()
            except Exception as e_excel: # Catching a broader range of pandas/excel reader errors
                logger.error(f"Excel processing error for {file_id}: {e_excel}")
                return f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Excel: {e_excel}. ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù…Ù„Ù ØºÙŠØ± ØªØ§Ù„Ù."
        else:
            logger.warning(f"Unsupported mime_type '{mime_type}' for schema extraction from file ID {file_id}.")
            return None # Should not be called for unsupported types based on askdata_command logic

        if column_names:
            return column_names # Return list of column names
        else:
            # This case might occur if CSV is empty after header or Excel sheet has no columns
            logger.warning(f"No column names extracted for file ID {file_id} (MIME: {mime_type}).")
            return None # Return None if no columns found or error in specific parsing

    except HttpError as error:
        logger.error(f"HttpError during schema extraction (download) for file ID {file_id}: {error}")
        return None # Indicate error by returning None
    except Exception as e:
        logger.error(f"General error in get_schema_from_file_sync for file ID {file_id}: {e}", exc_info=True)
        return None # Indicate error by returning None

async def get_schema_from_file_async(gdrive_service, file_id: str, mime_type: str) -> list[str] | None: # Return type updated
    return await asyncio.to_thread(get_schema_from_file_sync, gdrive_service, file_id, mime_type)

# --- Funciones para BÃºsqueda de Datos en Archivos ---
def perform_actual_search_sync(gdrive_service, file_id: str, mime_type: str, column_name: str, match_type: str, search_value: str) -> pd.DataFrame | str:
    try:
        logger.info(f"Starting actual search: File ID {file_id}, Column '{column_name}', Match '{match_type}', Value '{search_value}'")
        # Download file content
        request = gdrive_service.files().get_media(fileId=file_id)
        file_content_stream = io.BytesIO()
        downloader = MediaIoBaseDownloader(file_content_stream, request)
        done = False
        while not done:
            status, done = downloader.next_chunk()
        file_bytes = file_content_stream.getvalue()
        logger.info(f"File {file_id} downloaded for search.")

        # Load DataFrame
        df = None
        if mime_type == 'text/csv':
            try:
                text_content = None
                common_encodings = ['utf-8', 'iso-8859-1', 'windows-1252', 'windows-1256']
                for encoding in common_encodings:
                    try:
                        text_content = file_bytes.decode(encoding)
                        logger.info(f"Decoded CSV {file_id} with {encoding} for search.")
                        break
                    except UnicodeDecodeError:
                        logger.debug(f"Failed to decode CSV {file_id} with {encoding} for search.")
                if text_content is None:
                    return "Ø®Ø·Ø£: ØªØ¹Ø°Ø± ÙÙƒ ØªØ±Ù…ÙŠØ² Ù…Ù„Ù CSV."
                df = pd.read_csv(io.StringIO(text_content))
            except Exception as e_csv_load:
                logger.error(f"Error loading CSV into DataFrame for search (File ID: {file_id}): {e_csv_load}")
                return f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù CSV: {e_csv_load}"
        elif mime_type in ['application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']:
            try:
                engine = 'openpyxl' if mime_type == 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet' else None
                df = pd.read_excel(io.BytesIO(file_bytes), engine=engine)
            except Exception as e_excel_load:
                logger.error(f"Error loading Excel into DataFrame for search (File ID: {file_id}): {e_excel_load}")
                return f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Excel: {e_excel_load}"
        else:
            return f"Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù {mime_type} ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ… Ù„Ù„Ø¨Ø­Ø«."

        if df is None or df.empty:
            return "Ø§Ù„Ù…Ù„Ù ÙØ§Ø±Øº Ø£Ùˆ ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."
        if column_name not in df.columns:
            return f"Ø§Ù„Ø¹Ù…ÙˆØ¯ '{column_name}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù…Ù„Ù."

        # Filter DataFrame
        filtered_df = None
        # For robust comparison, convert column to string for string operations,
        # and attempt numeric conversion for numeric operations.

        col_as_str = df[column_name].astype(str)
        search_value_lower = search_value.lower()

        if match_type == 'contains':
            filtered_df = df[col_as_str.str.lower().str.contains(search_value_lower, case=False, na=False)]
        elif match_type == 'exact': # Changed from 'equals'
            try: # Attempt numeric exact match
                num_search_value = pd.to_numeric(search_value)
                # Coerce errors in column to NaN, then compare. NaN will not equal num_search_value.
                filtered_df = df[pd.to_numeric(df[column_name], errors='coerce') == num_search_value]
            except ValueError: # Fallback to string exact match
                filtered_df = df[col_as_str.str.lower() == search_value_lower]
        elif match_type == 'not_contains':
            filtered_df = df[~col_as_str.str.lower().str.contains(search_value_lower, case=False, na=False)]
        elif match_type == 'not_exact': # Changed from 'not_equals'
            try: # Attempt numeric non-equality
                num_search_value = pd.to_numeric(search_value)
                # For non-equality, NaNs in the column should also be included unless explicitly handled.
                # Here, they won't match num_search_value, so they are effectively "not equal".
                filtered_df = df[pd.to_numeric(df[column_name], errors='coerce') != num_search_value]
            except ValueError: # Fallback to string non-equality
                filtered_df = df[col_as_str.str.lower() != search_value_lower]
        elif match_type == 'greater_than':
            try:
                num_search_value = pd.to_numeric(search_value)
                filtered_df = df[pd.to_numeric(df[column_name], errors='coerce') > num_search_value]
            except ValueError:
                return "Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¨Ø­Ø« Ù„Ù€ 'Ø£ÙƒØ¨Ø± Ù…Ù†' ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø±Ù‚Ù…ÙŠØ©."
        elif match_type == 'less_than':
            try:
                num_search_value = pd.to_numeric(search_value)
                filtered_df = df[pd.to_numeric(df[column_name], errors='coerce') < num_search_value]
            except ValueError:
                return "Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¨Ø­Ø« Ù„Ù€ 'Ø£ØµØºØ± Ù…Ù†' ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø±Ù‚Ù…ÙŠØ©."
        elif match_type == 'starts_with':
            filtered_df = df[col_as_str.str.startswith(search_value, case=False, na=False)]
        elif match_type == 'ends_with':
            filtered_df = df[col_as_str.str.endswith(search_value, case=False, na=False)]
        else:
            return f"Ù†ÙˆØ¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© '{match_type}' ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ."

        if filtered_df is None: # Should ideally not happen if all paths lead to assignment or error return
             logger.error(f"filtered_df remained None for match_type '{match_type}' - this indicates a logic flaw.")
             return "Ø®Ø·Ø£ ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„ØªØ±. Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ù†ØªØ§Ø¦Ø¬."

        logger.info(f"Search for '{search_value}' in column '{column_name}' with match type '{match_type}' yielded {len(filtered_df)} results.")
        return filtered_df

    except HttpError as error:
        logger.error(f"HttpError during file download for search (File ID: {file_id}): {error}")
        return "Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù„Ù„Ø¨Ø­Ø«."
    except Exception as e:
        logger.error(f"General error in perform_actual_search_sync (File ID: {file_id}): {e}", exc_info=True)
        return f"Ø®Ø·Ø£ Ø¹Ø§Ù… ÙˆØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {type(e).__name__}"

async def perform_actual_search_async(gdrive_service, file_id: str, mime_type: str, column_name: str, match_type: str, search_value: str) -> pd.DataFrame | str:
    return await asyncio.to_thread(perform_actual_search_sync, gdrive_service, file_id, mime_type, column_name, match_type, search_value)

# --- (Ø¨Ù‚ÙŠØ© Ø§Ù„Ø¯ÙˆØ§Ù„ Ù…Ø«Ù„ start_command, echo_message, admin_test_command, generate_qr_image, qr_command_handler, get_openai_response, testai_command ÙƒÙ…Ø§ Ù‡ÙŠ) ---

# --- PDF Document Handler ---
async def handle_pdf_document(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info(f"Received a PDF file from user {update.effective_user.id} - Name: {update.message.document.file_name}")
    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)

    # Inform the user that processing has started
    processing_message = await update.message.reply_text("Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù PDFØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±...")

    file_id = update.message.document.file_id
    try:
        pdf_file = await context.bot.get_file(file_id)
        # Using download_as_bytearray first, then converting to bytes
        pdf_bytearray = await pdf_file.download_as_bytearray()
        pdf_bytes = bytes(pdf_bytearray)

        num_pages, extracted_text = await extract_text_from_pdf(pdf_bytes)

        if not extracted_text or extracted_text == "Error processing PDF.":
            reply_text = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù PDF Ù‡Ø°Ø§. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ù…Ù„Ù ÙØ§Ø±ØºÙ‹Ø§ØŒ ØªØ§Ù„ÙÙ‹Ø§ØŒ Ø£Ùˆ Ù…Ø­Ù…ÙŠÙ‹Ø§ Ø¨ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ±ØŒ Ø£Ùˆ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† ØµÙˆØ±Ø© Ù…Ù…Ø³ÙˆØ­Ø© Ø¶ÙˆØ¦ÙŠÙ‹Ø§ (ØªØ­ØªØ§Ø¬ OCR Ù„Ù… ÙŠØªÙ… ØªÙ†ÙÙŠØ°Ù‡ Ø¨Ø¹Ø¯)."
            logger.warning(f"PDF processing failed or returned empty/error text for file_id: {file_id}")
        else:
            # Escape for MarkdownV2 - This basic escape might need to be more robust
            # It's generally safer to avoid Markdown if the text can be complex, or use HTML parse mode
            def escape_md_v2(text: str) -> str:
                # Basic set of characters to escape for MarkdownV2
                escape_chars = r'_*[]()~`>#+-.!{}='
                return "".join(f'\\{char}' if char in escape_chars else char for char in text)

            text_preview_escaped = escape_md_v2(extracted_text[:200])

            reply_text = (
                f"ØªÙ…Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù PDF Ø¨Ù†Ø¬Ø§Ø­\\.\n"
                f"Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª: {num_pages}\n\n"
                f"Ø£ÙˆÙ„ 200 Ø­Ø±Ù Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ù„Øµ:\n"
                f"{text_preview_escaped}"
            )
            logger.info(f"Successfully extracted text from PDF file_id: {file_id}. Pages: {num_pages}. Preview: {extracted_text[:50]}...")

        # Edit the "Processing..." message with the result
        await context.bot.edit_message_text(chat_id=processing_message.chat_id, message_id=processing_message.message_id, text=reply_text, parse_mode='MarkdownV2')

    except Exception as e:
        logger.error(f"Error in handle_pdf_document for file_id {file_id}: {type(e).__name__} - {e}")
        # Edit the "Processing..." message with an error
        await context.bot.edit_message_text(chat_id=processing_message.chat_id, message_id=processing_message.message_id, text="Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù PDF.")

# --- Utility function to escape text for MarkdownV2 ---
def escape_markdown_v2(text: str) -> str:
    """Escapes special characters for Telegram MarkdownV2."""
    # Ensure text is a string
    if not isinstance(text, str):
        text = str(text)
    escape_chars = r'_*[]()~`>#+-.!{}='
    return "".join(f'\\{char}' if char in escape_chars else char for char in text)

# --- Generic Document Handler for Uploading to GDrive ---
async def handle_document_upload(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not DRIVE_BOT_UPLOAD_FOLDER_ID or DRIVE_BOT_UPLOAD_FOLDER_ID.strip() == "":
        logger.error("DRIVE_BOT_UPLOAD_FOLDER_ID is not set. Cannot upload file.")
        await update.message.reply_text("Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… ÙŠØªÙ… ØªØ¹ÙŠÙŠÙ† Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø±ÙØ¹ ÙÙŠ Google Drive. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù…Ø³Ø¤ÙˆÙ„ Ø§Ù„Ø¨ÙˆØª.")
        return

    doc = update.message.document
    file_id = doc.file_id
    original_file_name = doc.file_name or "untitled_document" # Fallback if file_name is None
    mime_type = doc.mime_type or "application/octet-stream"  # Default MIME type

    logger.info(f"Received document: '{original_file_name}' (MIME: {mime_type}, ID: {file_id}) for GDrive upload.")

    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.UPLOAD_DOCUMENT)

    try:
        bot_file = await context.bot.get_file(file_id)
        file_bytes_array = await bot_file.download_as_bytearray()
        file_bytes = bytes(file_bytes_array)

        service = await get_gdrive_service_async()
        if not service:
            await update.message.reply_text("ØªØ¹Ø°Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Google Drive. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù.")
            logger.error("Failed to get Google Drive service in handle_document_upload.")
            return

        upload_link = await upload_to_gdrive_async(service, file_bytes, original_file_name, mime_type, DRIVE_BOT_UPLOAD_FOLDER_ID)

        if upload_link:
            escaped_file_name = escape_markdown_v2(original_file_name)
            # The link itself is a URL, typically doesn't need escaping for the URL part of Markdown [text](url)
            # However, if the link text (which is also the filename here) has special chars, it needs escaping.
            reply_text = f"ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù '{escaped_file_name}' Ø¨Ù†Ø¬Ø§Ø­ Ø¥Ù„Ù‰ Google Drive\\.\nÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„ÙŠÙ‡ Ø¹Ø¨Ø± Ø§Ù„Ø±Ø§Ø¨Ø·: [{escaped_file_name}]({upload_link})"
            await update.message.reply_text(reply_text, parse_mode='MarkdownV2')
        else:
            escaped_file_name = escape_markdown_v2(original_file_name)
            reply_text = f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø­Ø§ÙˆÙ„Ø© Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù '{escaped_file_name}' Ø¥Ù„Ù‰ Google Drive\\."
            await update.message.reply_text(reply_text, parse_mode='MarkdownV2')

    except Exception as e:
        logger.error(f"Unexpected error in handle_document_upload for '{original_file_name}': {type(e).__name__} - {e}")
        escaped_file_name = escape_markdown_v2(original_file_name)
        await update.message.reply_text(f"Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù '{escaped_file_name}'\\. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ù‹Ø§\\.", parse_mode='MarkdownV2')


async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    welcome_message = (
        f"Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙŠØ§ {user.first_name}!\n"
        "Ø£Ù†Ø§ Ø¨ÙˆØªÙƒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø¨Ø§Ù„Ø£Ø³ÙÙ„ Ù„Ù„ØªÙ†Ù‚Ù„ Ø¨ÙŠÙ† Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø£Ùˆ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ù…Ø¨Ø§Ø´Ø±Ø©.\n\n"
        "Ø¨Ø¹Ø¶ Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©:\n"
        "â–«ï¸ /qr <Ù†Øµ> - Ù„Ø¥Ù†Ø´Ø§Ø¡ QR Code\n"
        "â–«ï¸ /testai <Ø³Ø¤Ø§Ù„> - Ù„Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„ Ø¹Ù„Ù‰ OpenAI\n"
        # The buttons will now represent these actions primarily
        # "â–«ï¸ /gdrivefiles - Ù„Ø¹Ø±Ø¶ Ù…Ù„ÙØ§Øª Google Drive\n"
        # "â–«ï¸ /askdata <Ø³Ø¤Ø§Ù„> - Ù„Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø¨ÙŠØ§Ù†Ø§ØªÙƒ\n"
        "This bot is being enhanced by Jules!\n"
    )

    # Define the main reply keyboard
    main_keyboard_layout = [
        [KeyboardButton("ğŸ—‚ï¸ Ù…Ù„ÙØ§ØªÙŠ ÙÙŠ Drive"), KeyboardButton("ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© PDF")],
        [KeyboardButton("ğŸ“¤ Ø±ÙØ¹ Ù…Ù„Ù Ø¥Ù„Ù‰ Drive"), KeyboardButton("â“ Ø§Ø³Ø£Ù„ Ø¹Ù† Ø¨ÙŠØ§Ù†Ø§ØªÙŠ")]
        # Future buttons like "âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª" or "ğŸ†˜ Ù…Ø³Ø§Ø¹Ø¯Ø©" can be added here
    ]
    reply_markup = ReplyKeyboardMarkup(
        main_keyboard_layout,
        resize_keyboard=True,
        one_time_keyboard=False # Keep the keyboard open until explicitly closed
    )

    await context.bot.send_message(
        chat_id=update.effective_chat.id,
        text=welcome_message,
        reply_markup=reply_markup
    )
    logger.info(f"User {user.id} ({user.first_name}) started the bot and received main keyboard.")

# --- Handlers for Reply Keyboard Buttons ---
async def prompt_pdf_upload(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info(f"User {update.effective_user.id} clicked 'ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© PDF' button.")
    await update.message.reply_text("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ù PDF Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡.")

async def prompt_general_upload(update: Update, context: ContextTypes.DEFAULT_TYPE):
    logger.info(f"User {update.effective_user.id} clicked 'ğŸ“¤ Ø±ÙØ¹ Ù…Ù„Ù Ø¥Ù„Ù‰ Drive' button.")
    await update.message.reply_text("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø±ÙØ¹Ù‡ Ø¥Ù„Ù‰ Google Drive.")

# Note: For "ğŸ—‚ï¸ Ù…Ù„ÙØ§ØªÙŠ ÙÙŠ Drive" and "â“ Ø§Ø³Ø£Ù„ Ø¹Ù† Ø¨ÙŠØ§Ù†Ø§ØªÙŠ",
# we will use Regex handlers to directly call the existing command functions.
# This requires those functions to be compatible.
# If they are not (e.g. they rely on context.args from a CommandHandler),
# then wrapper functions would be needed here as well.
# For now, we assume list_gdrive_files_command and askdata_command can be called.
# If askdata_command expects args, it will reply with its usage message.

async def echo_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_message = update.message.text
    reply_text = f"Ø£Ù†Øª Ù‚Ù„Øª: {user_message}"
    await context.bot.send_message(chat_id=update.effective_chat.id, text=reply_text)
    logger.info(f"User {update.effective_user.id} sent text: '{user_message}', bot echoed.")

async def admin_test_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    if user_id in ADMIN_IDS:
        reply_text = f"Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ {update.effective_user.first_name}! Ù‡Ø°Ø§ Ø£Ù…Ø± Ø®Ø§Øµ Ø¨Ùƒ."
    else:
        reply_text = "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± Ù…Ø®ØµØµ Ù„Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠÙ† ÙÙ‚Ø·."
    await context.bot.send_message(chat_id=update.effective_chat.id, text=reply_text)

async def generate_qr_image(text_to_encode: str) -> io.BytesIO | None:
    if not text_to_encode:
        return None
    qr_img = qrcode.make(text_to_encode)
    img_byte_arr = io.BytesIO()
    qr_img.save(img_byte_arr, format='PNG')
    img_byte_arr.seek(0)
    return img_byte_arr

async def qr_command_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not context.args:
        await update.message.reply_text("Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù…Ø±ØŒ Ø£Ø±Ø³Ù„: /qr <Ø§Ù„Ù†Øµ Ø£Ùˆ Ø§Ù„Ø±Ø§Ø¨Ø·>")
        return
    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.UPLOAD_PHOTO)
    text_to_encode = " ".join(context.args)
    qr_image_stream = await generate_qr_image(text_to_encode)
    if qr_image_stream:
        await update.message.reply_photo(photo=qr_image_stream, caption=f"QR Code Ù„Ù€: {text_to_encode}")
    else:
        await update.message.reply_text("Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ QR Code. ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ.")

# This is the old get_openai_response, which will be replaced by the one defined above.
# async def get_openai_response(api_key: str, user_question: str) -> str:
#     try:
#         logger.info(f"Sending request to OpenAI API with question: {user_question}")
#         def generate_sync():
#             client = OpenAI(api_key=api_key)
#             completion = client.chat.completions.create(
#                 model="gpt-4o-mini",
#                 messages=[
#                     {"role": "system", "content": "You are a helpful assistant. Please respond in Arabic unless the user asks for another language."},
#                     {"role": "user", "content": user_question}
#                 ],
#                 max_tokens=300
#             )
#             return completion.choices[0].message.content.strip()
#         assistant_reply = await asyncio.to_thread(generate_sync)
#         logger.info("Received response from OpenAI API.")
#         return assistant_reply if assistant_reply else "Ù„Ù… Ø£ØªÙ„Ù‚ Ø±Ø¯Ù‹Ø§ Ù†ØµÙŠÙ‹Ø§ Ù…Ù† OpenAI."
#     except Exception as e:
#         logger.error(f"Error communicating with OpenAI API: {e}")
#         error_message = str(e).lower()
#         if "invalid api key" in error_message or "incorrect api key" in error_message:
#              return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù…ÙØªØ§Ø­ OpenAI API ØºÙŠØ± ØµØ§Ù„Ø­. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù†Ù‡."
#         if "quota" in error_message or "rate limit" in error_message:
#             return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù‚Ø¯ ØªØ¬Ø§ÙˆØ²Øª Ø­ØµØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ùˆ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·Ù„Ø¨Ø§Øª Ù„Ù€ OpenAI API. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ù‹Ø§."
#         return f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ OpenAI: ({type(e).__name__})"

async def testai_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not context.args:
        await update.message.reply_text("Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù…Ø±ØŒ Ø£Ø±Ø³Ù„: /testai <Ø³Ø¤Ø§Ù„Ùƒ Ù„Ù€ OpenAI>")
        return
    user_question = " ".join(context.args)
    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)
    thinking_message = await update.message.reply_text("Ù„Ø­Ø¸Ø§ØªØŒ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ OpenAI... ğŸ§ ")
    logger.info(f"User {update.effective_user.id} asked OpenAI (via /testai): '{user_question}'")
    if not OPENAI_API_KEY:
        reply_text = "Ù…ÙØªØ§Ø­ OpenAI API ØºÙŠØ± Ù…ÙØ¹Ø¯ ÙÙŠ Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©."
        final_markup = None
        logger.error("OPENAI_API_KEY is not set.")
    else:
        # Construct messages list for the new get_openai_response
        messages_for_testai = [
            {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ÙÙŠØ¯. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø§ Ù„Ù… ÙŠØ·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø£Ø®Ø±Ù‰."},
            {"role": "user", "content": user_question}
        ]
        reply_text = await get_openai_response(OPENAI_API_KEY, messages_for_testai)
        keyboard = [[ InlineKeyboardButton("ğŸ‘", callback_data='feedback_useful'), InlineKeyboardButton("ğŸ‘", callback_data='feedback_not_useful'),]]
        final_markup = InlineKeyboardMarkup(keyboard)
    try:
        await context.bot.edit_message_text(chat_id=thinking_message.chat_id, message_id=thinking_message.message_id, text=reply_text, reply_markup=final_markup)
    except Exception as e:
        logger.error(f"Error editing 'thinking' message: {e}.")
        await update.message.reply_text(reply_text, reply_markup=final_markup)
    logger.info("Sent OpenAI's response.")

# --- /askdata Command Handler ---
async def askdata_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if not context.args:
        if update.message and update.message.text and update.message.text.startswith("â“ Ø§Ø³Ø£Ù„ Ø¹Ù† Ø¨ÙŠØ§Ù†Ø§ØªÙŠ"):
            await update.message.reply_text("Ù…Ø§ Ù‡Ùˆ Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØµÙŠØºØ©: `/askdata Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§ --file Ø§Ø³Ù…_Ø§Ù„Ù…Ù„Ù.csv` Ù„ØªØ­Ø¯ÙŠØ¯ Ù…Ù„Ù Ù…Ø¹ÙŠÙ†.")
            return
        await update.message.reply_text("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø¹Ø¯ Ø§Ù„Ø£Ù…Ø±. Ù…Ø«Ø§Ù„: `/askdata Ù…Ø§ Ù‡Ùˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª --file data.csv`")
        return

    full_text_args = " ".join(context.args)

    # Parse question and filename
    user_question_text = full_text_args
    target_file_name_query = None
    match = re.search(r"(.+?)\s*--file\s+(.+)", full_text_args, re.IGNORECASE)
    if match:
        user_question_text = match.group(1).strip()
        target_file_name_query = match.group(2).strip()

    if not user_question_text: # Only --file was provided, or empty question
        await update.message.reply_text("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„Ùƒ Ù‚Ø¨Ù„ `--file`. Ù…Ø«Ø§Ù„: `/askdata Ù…Ø§ Ù‡Ùˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø¨ÙŠØ¹Ø§Øª --file data.csv`")
        return

    logger.info(f"User {update.effective_user.id} asked via /askdata: Question='{user_question_text}', File Query='{target_file_name_query}'")
    thinking_message = await update.message.reply_text(f"Ù„Ø­Ø¸Ø§ØªØŒ Ø£ÙÙƒØ± ÙÙŠ Ø³Ø¤Ø§Ù„Ùƒ: \"{escape_markdown_v2(user_question_text)}\"...")

    if not OPENAI_API_KEY:
        await context.bot.edit_message_text(chat_id=thinking_message.chat_id, message_id=thinking_message.message_id, text="Ù…ÙØªØ§Ø­ OpenAI API ØºÙŠØ± Ù…ÙØ¹Ø¯.")
        logger.error("OPENAI_API_KEY is not set.")
        return

    target_file_id_for_openai = None
    target_mime_type_for_openai = None
    schema_description_for_openai = None
    file_context_message = "" # Additional context for OpenAI based on file

    if target_file_name_query:
        await context.bot.edit_message_text(chat_id=thinking_message.chat_id, message_id=thinking_message.message_id, text=f"Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙÙƒ '{escape_markdown_v2(target_file_name_query)}' ÙÙŠ Google Drive...")
        service = await get_gdrive_service_async()
        if not service:
            await context.bot.edit_message_text(chat_id=thinking_message.chat_id, message_id=thinking_message.message_id, text="ØªØ¹Ø°Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Google Drive Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„Ù.")
            # Fallback to answering without file context
        else:
            found_file_id, found_file_name, found_mime_type = await find_file_in_gdrive_async(service, target_file_name_query)

            if found_file_id:
                await context.bot.edit_message_text(chat_id=thinking_message.chat_id, message_id=thinking_message.message_id, text=f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù '{escape_markdown_v2(found_file_name)}'. Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ù„Ø§Øµ Ù…Ø®Ø·Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")

                # Check MIME type for compatibility (CSV/Excel)
                compatible_mime_types = ['text/csv', 'application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']
                if found_mime_type in compatible_mime_types:
                    target_file_id_for_openai = found_file_id
                    target_mime_type_for_openai = found_mime_type

                    schema_description_for_openai = await get_schema_from_file_async(service, target_file_id_for_openai, target_mime_type_for_openai)

                    if schema_description_for_openai:
                        file_context_message = f"\n\nØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø¨Ø§Ø³Ù… '{escape_markdown_v2(found_file_name)}' Ù…Ù† Ù†ÙˆØ¹ '{target_mime_type_for_openai}'. {schema_description_for_openai}."
                        await context.bot.edit_message_text(chat_id=thinking_message.chat_id, message_id=thinking_message.message_id, text=f"ØªÙ… Ø§Ø³ØªØ®Ù„Ø§Øµ Ù…Ø®Ø·Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù…Ù„Ù '{escape_markdown_v2(found_file_name)}'. Ø§Ù„Ø¢Ù† Ø£ÙÙƒØ± ÙÙŠ Ø¥Ø¬Ø§Ø¨Ø© Ø³Ø¤Ø§Ù„Ùƒ...")
                    else:
                        file_context_message = f"\n\nØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø¨Ø§Ø³Ù… '{escape_markdown_v2(found_file_name)}' ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ®Ù„Ø§Øµ Ù…Ø®Ø·Ø· Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©. Ø³Ø£Ø­Ø§ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø¯ÙˆÙ†Ù‡."
                        await context.bot.edit_message_text(chat_id=thinking_message.chat_id, message_id=thinking_message.message_id, text=file_context_message)
                else:
                    file_context_message = f"\n\nØ§Ù„Ù…Ù„Ù '{escape_markdown_v2(found_file_name)}' (Ù†ÙˆØ¹: {found_mime_type}) Ù„ÙŠØ³ Ù…Ù† Ù†ÙˆØ¹ CSV Ø£Ùˆ Excel. Ø³Ø£Ø¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø¯ÙˆÙ† ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆØ§Ù‡."
                    await context.bot.edit_message_text(chat_id=thinking_message.chat_id, message_id=thinking_message.message_id, text=file_context_message)
            else:
                file_context_message = f"\n\nÙ„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø¨Ø§Ù„Ø§Ø³Ù… '{escape_markdown_v2(target_file_name_query)}' ÙÙŠ Google Drive. Ø³Ø£Ø­Ø§ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø¯ÙˆÙ† Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù„Ù."
                await context.bot.edit_message_text(chat_id=thinking_message.chat_id, message_id=thinking_message.message_id, text=file_context_message)

    # Construct the messages list for OpenAI
    system_message_content = "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ù‡Ø§. "
    if schema_description_for_openai: # This now implies a file was found and schema extracted
        system_message_content += (
            f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ³Ø£Ù„ Ø¹Ù† Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ù„Ù. {schema_description_for_openai}. "
            "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙƒÙ…Ø±Ø¬Ø¹ Ø£Ø³Ø§Ø³ÙŠ Ù„ÙÙ‡Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ø£Ùˆ Ø§Ù‚ØªØ±Ø§Ø­ ÙƒÙˆØ¯ Pandas Ù…Ù†Ø§Ø³Ø¨ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø°Ù„Ùƒ Ù…Ù„Ø§Ø¦Ù…Ù‹Ø§ Ù„Ù„Ø³Ø¤Ø§Ù„. "
            "Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØªØ·Ù„Ø¨ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù†ÙØ³Ù‡Ø§ (Ù…Ø«Ù„Ø§Ù‹ØŒ 'Ù…Ø§ Ù‡Ùˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø¹Ø±ØŸ')ØŒ ÙˆØ¶Ø­ Ø£Ù†Ùƒ Ù„Ø§ ØªØ³ØªØ·ÙŠØ¹ Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ ÙƒÙŠÙÙŠØ© Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ø­Ø³Ø§Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø¹Ø·Ø§Ø©."
        )
    elif target_file_name_query: # File was specified but not found, or schema not extracted
         system_message_content += (
             f"Ø­Ø§ÙˆÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ØªØ­Ø¯ÙŠØ¯ Ù…Ù„Ù Ø¨Ø§Ø³Ù… '{escape_markdown_v2(target_file_name_query)}' ÙˆÙ„ÙƒÙ† Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡ Ø£Ùˆ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ù„Ø§Øµ Ù…Ø®Ø·Ø·Ù‡. "
             "Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù†ØŒ Ø£Ùˆ Ø§Ø·Ù„Ø¨ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø£Ùˆ ØªØ­Ù…ÙŠÙ„Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø©."
         )
    else: # No file specified
        system_message_content += "Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙŠØ³Ø£Ù„ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ø§Ù…Ø§Ù‹ Ø¹Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. "

    system_message_content += (
        "Ø­Ø§ÙˆÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ù‡ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…ÙÙŠØ¯. "
        "Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ ÙŠØªØ·Ù„Ø¨ Ø¹Ù…Ù„ÙŠØ© Ø­Ø³Ø§Ø¨ÙŠØ© Ù…Ø¹Ù‚Ø¯Ø© Ø£Ùˆ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ„Ù… ÙŠÙƒÙ† Ù„Ø¯ÙŠÙƒ ÙˆØµÙˆÙ„ Ù…Ø¨Ø§Ø´Ø± Ù„Ù„Ù…Ù„Ù (Ø£Ùˆ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ù…Ù„Ù ØµØ§Ù„Ø­)ØŒ "
        "ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø£Ùˆ Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø°ÙŠ Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…. Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ØªÙˆÙØ±Ø©."
    )

    messages_for_askdata = [
        {"role": "system", "content": system_message_content},
        {"role": "user", "content": user_question_text}
    ]

    logger.info(f"Sending to OpenAI for /askdata with messages: {messages_for_askdata}")
    response = await get_openai_response(OPENAI_API_KEY, messages_for_askdata)

    try:
        await context.bot.edit_message_text(
            chat_id=thinking_message.chat_id,
            message_id=thinking_message.message_id,
            text=response
            # TODO: Consider adding feedback buttons similar to testai_command later
        )
    except Exception as e:
        logger.error(f"Error editing 'thinking' message for /askdata: {e}")
        # Fallback to sending a new message if editing fails
        await update.message.reply_text(response)


# --- Ù…Ø¹Ø§Ù„Ø¬ Ø£Ù…Ø± Google Drive ---
async def list_gdrive_files_command(update: Update, context: ContextTypes.DEFAULT_TYPE, page_token: str = None, folder_id: str = 'root'):
    chat_id = update.effective_chat.id
    user_id = update.effective_user.id
    is_callback = hasattr(update, 'callback_query') and update.callback_query is not None
    message_to_edit_id = None

    if is_callback:
        message_to_edit_id = update.callback_query.message.message_id
        # Ensure query is answered for callbacks
        try:
            await update.callback_query.answer()
        except Exception as e_ans: # Can fail if already answered or too old
            logger.debug(f"Callback query answer failed (likely already answered): {e_ans}")
    else: # Initial command call
        await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)

    if not os.path.exists(CLIENT_SECRET_FILE):
        error_msg = f"Ù…Ù„Ù `{CLIENT_SECRET_FILE}` ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯."
        if is_callback and message_to_edit_id: await context.bot.edit_message_text(chat_id=chat_id, message_id=message_to_edit_id, text=error_msg, parse_mode='MarkdownV2')
        else: await context.bot.send_message(chat_id=chat_id, text=error_msg, parse_mode='MarkdownV2')
        logger.error(f"Missing {CLIENT_SECRET_FILE} at {os.getcwd()}")
        return

    if not os.path.exists(TOKEN_FILE) and not is_callback : # Only prompt for auth on direct command, not on page navigation
        await context.bot.send_message(chat_id, "Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù€ Google DriveØŒ Ø£Ø­ØªØ§Ø¬ Ø¥Ø°Ù†Ùƒ (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©). Ø§ØªØ¨Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª ÙÙŠ Ø§Ù„Ø·Ø±ÙÙŠØ©.")
        # Actual auth flow happens in get_gdrive_service_async if token is missing/invalid

    service = await get_gdrive_service_async()
    if not service:
        error_msg = "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Google Drive."
        if is_callback and message_to_edit_id: await context.bot.edit_message_text(chat_id=chat_id, message_id=message_to_edit_id, text=error_msg)
        else: await context.bot.send_message(chat_id=chat_id, text=error_msg)
        return

    try:
        logger.info(f"User {user_id} requested GDrive files. Folder: '{folder_id}', PageToken: '{page_token}'")

        drive_query = f"'{folder_id}' in parents and trashed=false"
        page_size = 10 # Number of files per page

        def list_files_sync():
            return service.files().list(
                q=drive_query,
                pageSize=page_size,
                fields="nextPageToken, files(id, name, mimeType, webViewLink)",
                orderBy="folder, name", # Folders first, then by name
                pageToken=page_token
            ).execute()

        results = await asyncio.to_thread(list_files_sync)
        items = results.get('files', [])
        next_page_token_from_api = results.get('nextPageToken')

        message_text_parts = []
        keyboard_buttons_rows = [] # Each element is a row of buttons

        if not items and not page_token: # Only show "empty" if it's the first page and no items
            message_text_parts.append("Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙØ§Ø±Øº Ø£Ùˆ Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª ÙŠÙ…ÙƒÙ† Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„ÙŠÙ‡Ø§.")
        else:
            if not is_callback: # For initial call, send a header message
                 message_text_parts.append("Ø£Ø­Ø¯Ø« Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØ§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª:") # This might be replaced if editing

            for item in items:
                icon = "ğŸ“" if item['mimeType'] == 'application/vnd.google-apps.folder' else "ğŸ“„"
                link = item.get('webViewLink', '') # Not used directly in message to save space, but good for logs
                file_name_escaped = escape_markdown_v2(item['name'])

                # Each file/folder will have its own message or be part of a list in one message.
                # For simplicity with inline buttons, sending one message per item is easier if buttons are complex.
                # However, to use pagination buttons effectively, all items for a page should be in ONE message.

                file_info_line = f"{icon} {file_name_escaped}"
                message_text_parts.append(file_info_line)

                action_buttons = []
                if item['mimeType'] != 'application/vnd.google-apps.folder':
                    action_buttons.append(InlineKeyboardButton("ğŸ“¥ ØªØ­Ù…ÙŠÙ„", callback_data=f'download_gdrive_{item["id"]}'))

                if item['mimeType'] == 'text/csv':
                    action_buttons.append(InlineKeyboardButton("ğŸ“„ Ø§Ù‚Ø±Ø£ CSV", callback_data=f'read_csv_{item["id"]}'))

                if item['mimeType'] == 'application/json' or item['name'].lower().endswith('.json'):
                    action_buttons.append(InlineKeyboardButton("ğŸ“Š ØªØ­Ù„ÙŠÙ„ JSON", callback_data=f'analyze_json_gdrive_{item["id"]}'))

                if item['mimeType'] == 'text/plain' or item['name'].lower().endswith('.txt'):
                    action_buttons.append(InlineKeyboardButton("ğŸ“– Ø¹Ø±Ø¶ TXT", callback_data=f'options_txt_gdrive_{item["id"]}'))

                if action_buttons:
                    keyboard_buttons_rows.append(action_buttons) # Add row of buttons for this file

        # Pagination buttons
        pagination_row = []
        # Simplified: No "Previous" button for now due to GDrive API limitations (no prevPageToken)
        # If we were on page > 1 (i.e., page_token was not None), a "Previous" button could be constructed
        # if we stored the token that led to the current page.

        if next_page_token_from_api:
            pagination_row.append(InlineKeyboardButton("Ø§Ù„ØªØ§Ù„ÙŠ âª", callback_data=f"gdrive_page_{folder_id}_{next_page_token_from_api}"))

        if pagination_row:
            keyboard_buttons_rows.append(pagination_row)

        final_message_text = "\n".join(message_text_parts)
        if not final_message_text: # Should not happen if logic is correct
            final_message_text = "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ØµØ± Ù„Ø¹Ø±Ø¶Ù‡Ø§."

        final_reply_markup = InlineKeyboardMarkup(keyboard_buttons_rows) if keyboard_buttons_rows else None

        if is_callback and message_to_edit_id:
            await context.bot.edit_message_text(
                chat_id=chat_id,
                message_id=message_to_edit_id,
                text=final_message_text,
                reply_markup=final_reply_markup,
                parse_mode='MarkdownV2'
            )
        else:
            await context.bot.send_message(
                chat_id=chat_id,
                text=final_message_text,
                reply_markup=final_reply_markup,
                parse_mode='MarkdownV2'
            )

    except HttpError as http_err:
        logger.error(f'HttpError listing GDrive files: {http_err}. Response: {http_err.content}')
        error_message = f'Ø®Ø·Ø£ ÙÙŠ Ø¹Ø±Ø¶ Ù…Ù„ÙØ§Øª GDrive \\({http_err.resp.status}\\)\\. Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø© Ø¥Ø°Ø§ Ø§Ù†ØªÙ‡Øª ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ù…ÙŠØ²\\. Ø¬Ø±Ø¨ Ø­Ø°Ù `token\\.json` ÙˆØ¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ù…Ø±\\.'
        if is_callback and message_to_edit_id: await context.bot.edit_message_text(chat_id=chat_id, message_id=message_to_edit_id, text=error_message, parse_mode='MarkdownV2')
        else: await context.bot.send_message(chat_id=chat_id, text=error_message, parse_mode='MarkdownV2')
    except Exception as e:
        logger.error(f'Error listing GDrive files: {e}', exc_info=True)
        error_message = f'Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø¹Ø±Ø¶ Ù…Ù„ÙØ§Øª GDrive: {escape_markdown_v2(str(type(e).__name__))}'
        if is_callback and message_to_edit_id: await context.bot.edit_message_text(chat_id=chat_id, message_id=message_to_edit_id, text=error_message, parse_mode='MarkdownV2')
        else: await context.bot.send_message(chat_id=chat_id, text=error_message, parse_mode='MarkdownV2')


# --- !!! Ø¯Ø§Ù„Ø© button_callback Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ© (Ù„ÙŠØ³Øª Ø§Ù„ØªØ´Ø®ÙŠØµÙŠØ©) !!! ---
async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """ÙŠØ¹Ø§Ù„Ø¬ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ù…Ø¶Ù…Ù†Ø© Ù„Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø®ØªÙ„ÙØ©."""
    query = update.callback_query
    # Ù†Ø±Ø¯ Ø¹Ù„Ù‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… ÙÙˆØ±Ù‹Ø§ Ù„ØªØ¬Ù†Ø¨ Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù‡Ù„Ø© Ø§Ù„Ù€ callback
    await query.answer() 

    user_id = query.from_user.id
    callback_data = query.data
    
    logger.info(f"--- BUTTON CALLBACK (Functional) --- User ID: {user_id}, Callback Data: '{callback_data}'")
    
    chat_id_to_reply = query.message.chat_id if query.message else user_id

    if callback_data.startswith('read_csv_'):
        file_id = callback_data.split('_', 2)[2]
        logger.info(f"User {user_id} requested to read CSV file with ID: {file_id}")
        
        try:
            await query.edit_message_text(text=f"{query.message.text if query.message else ''}\n\n---\nØ¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù CSV (ID: {file_id})...")
        except Exception as e:
            logger.warning(f"Could not edit original button message for read_csv_ action: {e}")
            await context.bot.send_message(chat_id=chat_id_to_reply, text=f"Ø¬Ø§Ø±ÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù CSV (ID: {file_id})...")
        
        await context.bot.send_chat_action(chat_id=chat_id_to_reply, action=ChatAction.TYPING)

        service = await get_gdrive_service_async()
        if not service:
            await context.bot.send_message(chat_id=chat_id_to_reply, text="ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Google Drive. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù.")
            return
        
        # --- Ø¯Ø§Ù„Ø© Ø¯Ø§Ø®Ù„ÙŠØ© Ù„ØªÙ†Ø²ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© CSV Ø¨Ø´ÙƒÙ„ Ù…ØªØ²Ø§Ù…Ù† ---
        def download_and_process_csv_sync(gdrive_service, gdrive_file_id):
            try:
                logger.info(f"Attempting to download file with ID: {gdrive_file_id}")
                request = gdrive_service.files().get_media(fileId=gdrive_file_id)
                csv_content_stream = io.BytesIO()
                csv_content_stream.write(request.execute())
                csv_content_stream.seek(0)
                logger.info(f"Successfully downloaded content for file ID: {gdrive_file_id}")
                
                df = pd.read_csv(csv_content_stream)
                if df.empty:
                    return "Ù…Ù„Ù CSV ÙØ§Ø±Øº Ø£Ùˆ Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡ØªÙ‡Ø§."
                
                num_rows, num_cols = df.shape
                columns = ", ".join(df.columns.tolist())
                
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… backticks Ø«Ù„Ø§Ø«ÙŠØ© Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ÙƒÙˆØ¯ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ ÙÙŠ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…
                output_message_unescaped = (
                    f"ØªÙ…Øª Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù CSV (ID: {gdrive_file_id}) Ø¨Ù†Ø¬Ø§Ø­!\n"
                    f"Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ: {num_rows}\n"
                    f"Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: {num_cols}\n"
                    f"Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©: `{columns}`\n\n"
                    f"Ø£ÙˆÙ„ 3 ØµÙÙˆÙ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n"
                    f"```\n{df.head(3).to_string(index=True)}\n```"
                )
                # Ù„Ø§ Ø­Ø§Ø¬Ø© Ù„Ø¯Ø§Ù„Ø© ØªØ·Ù‡ÙŠØ± Ù…Ù†ÙØµÙ„Ø© Ù‡Ù†Ø§ØŒ Ø³Ù†Ø³ØªØ®Ø¯Ù… parse_mode='MarkdownV2' Ø¨Ø­Ø°Ø±
                # ÙˆØ³Ù†Ù‡ØªÙ… Ø¨ØªØ·Ù‡ÙŠØ± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø­Ø±Ù Ø®Ø§ØµØ© Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                return output_message_unescaped

            except pd.errors.EmptyDataError:
                logger.warning("Attempted to read an empty CSV file or stream (EmptyDataError).")
                return "Ù…Ù„Ù CSV ÙØ§Ø±Øº Ø£Ùˆ Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª."
            except HttpError as http_err: # Ø®Ø·Ø£ Ù…Ù† Google API Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ø²ÙŠÙ„
                logger.error(f"HttpError downloading/processing CSV {gdrive_file_id}: {http_err}")
                return f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø²ÙŠÙ„ Ù…Ù„Ù CSV (ID: {gdrive_file_id}): {http_err.resp.status}"
            except Exception as e:
                logger.error(f"Error processing CSV data with pandas for {gdrive_file_id}: {e}")
                return f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù CSV (ID: {gdrive_file_id}): {type(e).__name__}"
        # --- Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© ---

        analysis_result = await asyncio.to_thread(download_and_process_csv_sync, service, file_id)
        
        # Ø¥Ø±Ø³Ø§Ù„ Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙƒØ±Ø³Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©
        # ØªØ£ÙƒØ¯ Ù…Ù† ØªØ·Ù‡ÙŠØ± Ø§Ù„Ù†Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø³ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø­Ø±Ù Markdown Ø®Ø§ØµØ©
        # The escape_markdown_v2 function is already globally defined, so we can use it directly.
        # def escape_markdown_v2(text: str) -> str:
        #     escape_chars = r'_*[]()~`>#+-.!{}='
        #     return "".join(f'\\{char}' if char in escape_chars else char for char in text)

        await context.bot.send_message(chat_id=chat_id_to_reply, text=escape_markdown_v2(analysis_result), parse_mode='MarkdownV2')

    elif callback_data.startswith('download_gdrive_'):
        file_id = callback_data.split('_', 2)[2]
        logger.info(f"User {user_id} requested to download GDrive file ID: {file_id}")
        original_message_text = query.message.text if query.message else "ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ù„Ù..."

        try:
            await query.edit_message_text(text=f"{original_message_text}\n\n---\nğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ù„Ù Ù„Ù„ØªÙ†Ø²ÙŠÙ„...")
        except Exception as e:
            logger.warning(f"Could not edit original button message for download_gdrive_ action: {e}")
            await context.bot.send_message(chat_id=chat_id_to_reply, text="ğŸ“¥ Ø¬Ø§Ø±ÙŠ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ù„Ù Ù„Ù„ØªÙ†Ø²ÙŠÙ„...")

        await context.bot.send_chat_action(chat_id=chat_id_to_reply, action=ChatAction.UPLOAD_DOCUMENT)

        service = await get_gdrive_service_async()
        if not service:
            await context.bot.send_message(chat_id=chat_id_to_reply, text="ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Google Drive. Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù.")
            # Attempt to revert the "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ¬Ù‡ÙŠØ²" message if possible, or send a new one
            try:
                await query.edit_message_text(text=original_message_text + "\n\n---\nâŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø¯Ù…Ø©.")
            except: pass # Ignore if editing fails
            return

        def download_gdrive_file_sync(gdrive_service, gdrive_file_id_sync) -> tuple[str | None, bytes | None]:
            try:
                file_metadata = gdrive_service.files().get(fileId=gdrive_file_id_sync, fields='name, mimeType').execute()
                original_file_name_sync = file_metadata.get('name')
                # mime_type_sync = file_metadata.get('mimeType') # mime_type not strictly needed for sending

                request = gdrive_service.files().get_media(fileId=gdrive_file_id_sync)
                file_content_stream = io.BytesIO()
                downloader = MediaIoBaseDownloader(file_content_stream, request)
                done = False
                while not done:
                    status, done = downloader.next_chunk()
                    if status:
                        logger.info(f"GDrive Download {gdrive_file_id_sync}: {int(status.progress() * 100)}%.")

                file_bytes_sync = file_content_stream.getvalue()
                logger.info(f"Successfully downloaded GDrive file ID: {gdrive_file_id_sync}, Name: {original_file_name_sync}")
                return original_file_name_sync, file_bytes_sync
            except HttpError as http_err_sync:
                logger.error(f"HttpError downloading GDrive file {gdrive_file_id_sync}: {http_err_sync}")
                return None, None
            except Exception as e_sync:
                logger.error(f"General error downloading GDrive file {gdrive_file_id_sync}: {e_sync}")
                return None, None

        original_file_name, file_bytes = await asyncio.to_thread(download_gdrive_file_sync, service, file_id)

        if original_file_name and file_bytes:
            try:
                await context.bot.send_document(chat_id=chat_id_to_reply, document=file_bytes, filename=original_file_name)
                # Try to edit the original message to indicate completion or remove buttons
                await query.edit_message_text(text=f"{original_message_text}\n\n---\nâœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù: {escape_markdown_v2(original_file_name)}")
            except Exception as send_exc:
                logger.error(f"Error sending document or editing message after download for GDrive file ID {file_id}: {send_exc}")
                await context.bot.send_message(chat_id=chat_id_to_reply, text=f"ØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù '{escape_markdown_v2(original_file_name)}' ÙˆÙ„ÙƒÙ† Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„Ù‡ Ù„Ùƒ. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø£Ùˆ ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø®Ø§ØµØ©.")
        else:
            await context.bot.send_message(chat_id=chat_id_to_reply, text="Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø¯Ø¯ Ù…Ù† Google Drive.")
            # Attempt to revert the "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ¬Ù‡ÙŠØ²" message
            try:
                await query.edit_message_text(text=original_message_text + "\n\n---\nâŒ ÙØ´Ù„ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„Ù.")
            except: pass

    elif callback_data.startswith('analyze_json_gdrive_'):
        file_id = callback_data.split('_', 3)[3] # analyze_json_gdrive_FILEID
        logger.info(f"User {user_id} requested to analyze GDrive JSON file ID: {file_id}")
        original_message_text = query.message.text if query.message else "ØªØ­Ù„ÙŠÙ„ JSON..."

        try:
            await query.edit_message_text(text=f"{original_message_text}\n\n---\nğŸ“Š Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù JSON...")
        except Exception as e:
            logger.warning(f"Could not edit original button message for analyze_json_gdrive_ action: {e}")
            await context.bot.send_message(chat_id=chat_id_to_reply, text="ğŸ“Š Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù JSON...")

        await context.bot.send_chat_action(chat_id=chat_id_to_reply, action=ChatAction.TYPING)

        service = await get_gdrive_service_async()
        if not service:
            await context.bot.send_message(chat_id=chat_id_to_reply, text="ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Google Drive. Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„Ù.")
            try: await query.edit_message_text(text=original_message_text + "\n\n---\nâŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø¯Ù…Ø©.")
            except: pass
            return

        def process_json_file_sync(gdrive_service, gdrive_file_id_sync: str) -> str:
            try:
                file_metadata = gdrive_service.files().get(fileId=gdrive_file_id_sync, fields='name, size').execute()
                original_file_name_sync = file_metadata.get('name', 'unknown.json')
                file_size = int(file_metadata.get('size', 0))

                # Limit file size to 5MB for now
                if file_size > 5 * 1024 * 1024: # 5MB
                    logger.warning(f"JSON file {gdrive_file_id_sync} ('{original_file_name_sync}') is too large: {file_size} bytes.")
                    return f"Ù…Ù„Ù JSON '{escape_markdown_v2(original_file_name_sync)}' ÙƒØ¨ÙŠØ± Ø¬Ø¯Ù‹Ø§ Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ({file_size / (1024*1024):.2f} MB)."

                request = gdrive_service.files().get_media(fileId=gdrive_file_id_sync)
                file_content_stream = io.BytesIO()
                downloader = MediaIoBaseDownloader(file_content_stream, request)
                done = False
                while not done:
                    status, done = downloader.next_chunk()
                    # logger.info(f"JSON Download {gdrive_file_id_sync}: {int(status.progress() * 100)}%.") # Can be verbose

                file_bytes_sync = file_content_stream.getvalue()

                try:
                    json_string = file_bytes_sync.decode('utf-8')
                except UnicodeDecodeError:
                    logger.error(f"UnicodeDecodeError for JSON file {gdrive_file_id_sync} ('{original_file_name_sync}').")
                    return f"Ø®Ø·Ø£ ÙÙŠ ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù„Ù '{escape_markdown_v2(original_file_name_sync)}'. ØªØ£ÙƒØ¯ Ø£Ù†Ù‡ Ø¨ØµÙŠØºØ© UTF-8."

                try:
                    data = json.loads(json_string)
                except json.JSONDecodeError as json_err:
                    logger.error(f"JSONDecodeError for file {gdrive_file_id_sync} ('{original_file_name_sync}'): {json_err}")
                    return f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ JSON Ù„Ù„Ù…Ù„Ù '{escape_markdown_v2(original_file_name_sync)}':\n`{escape_markdown_v2(str(json_err))}`"

                analysis_summary = f"*Ù…Ù„Ù JSON: {escape_markdown_v2(original_file_name_sync)}*\n"

                if isinstance(data, list):
                    try:
                        df = pd.json_normalize(data)
                        # df = pd.DataFrame(data) # Alternative for simple list of dicts
                        analysis_summary += f"ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù„Ù‰ Ø¬Ø¯ÙˆÙ„ Ø¨ÙŠØ§Ù†Ø§Øª \\({len(df)} ØµÙÙˆÙ, {len(df.columns)} Ø£Ø¹Ù…Ø¯Ø©\\)\\.\n"
                        analysis_summary += f"*Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:* `{escape_markdown_v2(', '.join(df.columns))}`\n"
                        if not df.empty:
                             analysis_summary += f"*Ø£ÙˆÙ„ 3 ØµÙÙˆÙ:*\n```\n{df.head(3).to_string()}\n```"
                        else:
                            analysis_summary += "Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù†Ø§ØªØ¬ ÙØ§Ø±Øº."
                    except Exception as e_df:
                        logger.error(f"Error converting JSON list to DataFrame for file '{original_file_name_sync}': {e_df}")
                        analysis_summary += "Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ù‚Ø¯Ø© ÙˆÙ„Ù… ÙŠØªÙ… ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ Ø¬Ø¯ÙˆÙ„ Ù‚ÙŠØ§Ø³ÙŠ\\. Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ù…Ø¹ÙŠÙ† Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©\\."

                elif isinstance(data, dict):
                    analysis_summary += "Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒØ§Ø¦Ù† JSON\\. *Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:* `" + escape_markdown_v2(", ".join(data.keys())) + "`\n"
                    # Try to find a list of records within the dictionary
                    processed_nested_list = False
                    for key, value in data.items():
                        if isinstance(value, list) and value and all(isinstance(i, dict) for i in value):
                            try:
                                df = pd.json_normalize(value)
                                analysis_summary += f"\nØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙØªØ§Ø­ '{escape_markdown_v2(key)}' ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ø¬Ø¯ÙˆÙ„ \\({len(df)} ØµÙÙˆÙ, {len(df.columns)} Ø£Ø¹Ù…Ø¯Ø©\\):\n"
                                analysis_summary += f"*Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:* `{escape_markdown_v2(', '.join(df.columns))}`\n"
                                if not df.empty:
                                    analysis_summary += f"*Ø£ÙˆÙ„ 3 ØµÙÙˆÙ:*\n```\n{df.head(3).to_string()}\n```\n"
                                else:
                                    analysis_summary += "Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù†Ø§ØªØ¬ Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø© ÙØ§Ø±Øº.\n"
                                processed_nested_list = True
                                break # Process first such list for now
                            except Exception as e_df_nested:
                                logger.error(f"Error converting nested list under key '{key}' to DataFrame for file '{original_file_name_sync}': {e_df_nested}")
                                analysis_summary += f"ØªØ¹Ø°Ø± ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø© ØªØ­Øª Ø§Ù„Ù…ÙØªØ§Ø­ '{escape_markdown_v2(key)}' Ø¥Ù„Ù‰ Ø¬Ø¯ÙˆÙ„ Ù‚ÙŠØ§Ø³ÙŠ.\n"
                    if not processed_nested_list:
                         analysis_summary += "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø³ÙŠØ·Ø© Ù…Ù† Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒØ§Ø¦Ù† Ù„ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ù„Ø¬Ø¯ÙˆÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§."
                else:
                    analysis_summary += "ØµÙŠØºØ© JSON ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ø­Ø§Ù„ÙŠÙ‹Ø§ \\(Ù„ÙŠØ³Øª Ù‚Ø§Ø¦Ù…Ø© Ø£Ùˆ ÙƒØ§Ø¦Ù† Ù‚ÙŠØ§Ø³ÙŠ\\)\\."

                return analysis_summary

            except HttpError as http_err_sync:
                logger.error(f"HttpError processing JSON file {gdrive_file_id_sync}: {http_err_sync}")
                return f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù…Ù„Ù JSON (ID: {gdrive_file_id_sync}) Ø¹Ù„Ù‰ Google Drive."
            except Exception as e_sync:
                logger.error(f"General error processing JSON file {gdrive_file_id_sync}: {e_sync}")
                return f"Ø®Ø·Ø£ Ø¹Ø§Ù… Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù JSON (ID: {gdrive_file_id_sync}): {type(e_sync).__name__}"

        analysis_result = await asyncio.to_thread(process_json_file_sync, service, file_id)

        try:
            await query.edit_message_text(text=analysis_result, parse_mode='MarkdownV2')
        except Exception as e_edit: # If message is too long or contains problematic markdown
            logger.warning(f"Failed to edit message with JSON analysis, sending as new: {e_edit}")
            await context.bot.send_message(chat_id=chat_id_to_reply, text=analysis_result, parse_mode='MarkdownV2')

    elif callback_data.startswith('options_txt_gdrive_'):
        file_id = callback_data.split('_', 3)[3] # options_txt_gdrive_FILEID
        logger.info(f"User {user_id} requested options for GDrive TXT file ID: {file_id}")

        # Fetch file name to display in the options message
        service = await get_gdrive_service_async()
        if not service:
            await query.edit_message_text(text="ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Google Drive. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø¹Ø±Ø¶ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…Ù„Ù.")
            return

        try:
            file_metadata = await asyncio.to_thread(service.files().get(fileId=file_id, fields='name').execute)
            original_file_name = file_metadata.get('name', 'file.txt')
        except Exception as e_meta:
            logger.error(f"Error fetching metadata for TXT options (ID: {file_id}): {e_meta}")
            await query.edit_message_text(text="Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª.")
            return

        txt_options_keyboard = [
            [InlineKeyboardButton(" Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 100 Ø³Ø·Ø±", callback_data=f"view_txt_head_{file_id}")],
            [InlineKeyboardButton(" Ø¹Ø±Ø¶ Ø¢Ø®Ø± 100 Ø³Ø·Ø±", callback_data=f"view_txt_tail_{file_id}")],
            # [InlineKeyboardButton("Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø© (Ù‚Ø±ÙŠØ¨Ù‹Ø§)", callback_data=f"search_txt_{file_id}_disabled")]
        ]
        options_markup = InlineKeyboardMarkup(txt_options_keyboard)
        try:
            await query.edit_message_text(
                text=f"Ø§Ø®ØªØ± Ø¥Ø¬Ø±Ø§Ø¡ Ù„Ù…Ù„Ù TXT: '{escape_markdown_v2(original_file_name)}'",
                reply_markup=options_markup,
                parse_mode='MarkdownV2'
            )
        except Exception as e_edit_options:
            logger.error(f"Error editing message to show TXT options: {e_edit_options}")
            # Fallback if edit fails (e.g. message too old, or content identical)
            await context.bot.send_message(
                chat_id=chat_id_to_reply,
                text=f"Ø§Ø®ØªØ± Ø¥Ø¬Ø±Ø§Ø¡ Ù„Ù…Ù„Ù TXT: '{escape_markdown_v2(original_file_name)}'",
                reply_markup=options_markup,
                parse_mode='MarkdownV2'
            )

    elif callback_data.startswith('view_txt_head_') or callback_data.startswith('view_txt_tail_'):
        mode = 'head' if callback_data.startswith('view_txt_head_') else 'tail'
        file_id = callback_data.split('_', 3)[3] # view_txt_head_FILEID or view_txt_tail_FILEID

        logger.info(f"User {user_id} requested to view {mode} of GDrive TXT file ID: {file_id}")

        # It's better to send a new message for the processing status,
        # rather than editing the message that contains the options keyboard.
        status_message = await context.bot.send_message(chat_id=chat_id_to_reply, text=f"ğŸ“– Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù TXT ({mode})...")
        await context.bot.send_chat_action(chat_id=chat_id_to_reply, action=ChatAction.TYPING)

        service = await get_gdrive_service_async()
        if not service:
            await context.bot.edit_message_text(chat_id=status_message.chat_id, message_id=status_message.message_id, text="ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Google Drive.")
            return

        def process_txt_file_sync(gdrive_service, gdrive_file_id_sync: str, view_mode: str) -> str:
            try:
                file_metadata = gdrive_service.files().get(fileId=gdrive_file_id_sync, fields='name, size').execute()
                original_file_name_sync = file_metadata.get('name', 'file.txt')
                file_size = int(file_metadata.get('size', 0))

                # Limit file size for viewing (e.g., 2MB)
                if file_size > 2 * 1024 * 1024: # 2MB
                    logger.warning(f"TXT file {gdrive_file_id_sync} ('{original_file_name_sync}') is too large for viewing: {file_size} bytes.")
                    return f"Ù…Ù„Ù TXT '{escape_markdown_v2(original_file_name_sync)}' ÙƒØ¨ÙŠØ± Ø¬Ø¯Ù‹Ø§ Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ({file_size / (1024*1024):.2f} MB)."

                request = gdrive_service.files().get_media(fileId=gdrive_file_id_sync)
                file_content_stream = io.BytesIO()
                downloader = MediaIoBaseDownloader(file_content_stream, request)
                done = False
                while not done:
                    status, done = downloader.next_chunk()

                file_bytes_sync = file_content_stream.getvalue()

                try:
                    # Try common encodings, starting with utf-8
                    encodings_to_try = ['utf-8', 'iso-8859-1', 'windows-1256'] # windows-1256 for Arabic
                    text_content = None
                    for enc in encodings_to_try:
                        try:
                            text_content = file_bytes_sync.decode(enc)
                            logger.info(f"Decoded TXT file {gdrive_file_id_sync} with {enc}")
                            break
                        except UnicodeDecodeError:
                            logger.debug(f"Failed to decode TXT {gdrive_file_id_sync} with {enc}")

                    if text_content is None:
                        logger.error(f"Could not decode TXT file {gdrive_file_id_sync} with common encodings.")
                        return f"Ø®Ø·Ø£ ÙÙŠ ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù„Ù '{escape_markdown_v2(original_file_name_sync)}'. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„ØªØ±Ù…ÙŠØ² ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…."

                except UnicodeDecodeError: # Should be caught by the loop above
                    logger.error(f"UnicodeDecodeError for TXT file {gdrive_file_id_sync} ('{original_file_name_sync}').")
                    return f"Ø®Ø·Ø£ ÙÙŠ ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù„Ù '{escape_markdown_v2(original_file_name_sync)}'."

                lines = text_content.splitlines()

                if view_mode == 'head':
                    selected_lines = lines[:100]
                    result_header = f"Ø£ÙˆÙ„ 100 Ø³Ø·Ø± Ù…Ù† Ù…Ù„Ù '{escape_markdown_v2(original_file_name_sync)}':\n"
                else: # tail
                    selected_lines = lines[-100:]
                    result_header = f"Ø¢Ø®Ø± 100 Ø³Ø·Ø± Ù…Ù† Ù…Ù„Ù '{escape_markdown_v2(original_file_name_sync)}':\n"

                output_text = "\n".join(selected_lines)

                if not output_text.strip():
                    return f"{result_header}\n(Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø­ØªÙˆÙ‰ Ù„Ø¹Ø±Ø¶Ù‡ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù…Ù„Ù)"

                # Truncate if too long for a Telegram message (4096 chars limit)
                max_len = 4000 # Leave some room for header and markdown code block
                if len(output_text) > max_len:
                    output_text = output_text[:max_len] + "\n... (ØªÙ… Ø§Ù‚ØªØ·Ø§Ø¹ Ø§Ù„Ù†Øµ Ù„Ø·ÙˆÙ„Ù‡ Ø§Ù„Ø²Ø§Ø¦Ø¯)"

                return f"{result_header}```\n{output_text}\n```" # Using Markdown code block

            except HttpError as http_err_sync:
                logger.error(f"HttpError processing TXT file {gdrive_file_id_sync}: {http_err_sync}")
                return f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù…Ù„Ù TXT (ID: {gdrive_file_id_sync}) Ø¹Ù„Ù‰ Google Drive."
            except Exception as e_sync:
                logger.error(f"General error processing TXT file {gdrive_file_id_sync}: {e_sync}")
                return f"Ø®Ø·Ø£ Ø¹Ø§Ù… Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù TXT (ID: {gdrive_file_id_sync}): {type(e_sync).__name__}"

        processed_text = await asyncio.to_thread(process_txt_file_sync, service, file_id, mode)

        # Edit the "Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©" message with the result or send a new one if it's too long / causes issues
        try:
             await context.bot.edit_message_text(chat_id=status_message.chat_id, message_id=status_message.message_id, text=processed_text, parse_mode='MarkdownV2')
        except Exception as e_final_edit:
            logger.warning(f"Failed to edit status message with TXT content, sending as new: {e_final_edit}")
            await context.bot.send_message(chat_id=chat_id_to_reply, text=processed_text, parse_mode='MarkdownV2')

    elif callback_data.startswith('gdrive_page_'):
        # Format: gdrive_page_FOLDERID_PAGETOKEN
        # If PAGETOKEN is 'None' (as string), it means first page of that FOLDERID
        parts = callback_data.split('_', 3)
        folder_id_from_cb = parts[2]
        next_token_from_cb = parts[3] if len(parts) > 3 else None
        if next_token_from_cb == 'None': # Handle explicit 'None' string if passed
            next_token_from_cb = None

        logger.info(f"User {user_id} requested GDrive page. Folder ID: {folder_id_from_cb}, Next Page Token: {next_token_from_cb}")
        # Call list_gdrive_files_command, making sure `update` is the CallbackQuery object
        await list_gdrive_files_command(update.callback_query, context, page_token=next_token_from_cb, folder_id=folder_id_from_cb)


    elif callback_data == 'feedback_useful':
        try:
            await query.edit_message_text(text=f"{query.message.text if query.message else 'ØªÙ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ…'}\n\n---\nØªÙ‚ÙŠÙŠÙ…Ùƒ: Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙÙŠØ¯Ø© ğŸ‘")
        except Exception as e: # Ù‚Ø¯ ØªÙØ´Ù„ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§ Ø£Ùˆ ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§ Ø¨Ø§Ù„ÙØ¹Ù„
            logger.warning(f"Could not edit message for useful feedback: {e}")
            await context.bot.send_message(chat_id=chat_id_to_reply, text="Ø´ÙƒØ±Ù‹Ø§ Ù„ØªÙ‚ÙŠÙŠÙ…Ùƒ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ! ğŸ‘")
            
    elif callback_data == 'feedback_not_useful':
        try:
            await query.edit_message_text(text=f"{query.message.text if query.message else 'ØªÙ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ…'}\n\n---\nØªÙ‚ÙŠÙŠÙ…Ùƒ: Ø¥Ø¬Ø§Ø¨Ø© ØºÙŠØ± Ù…ÙÙŠØ¯Ø© ğŸ‘")
        except Exception as e:
            logger.warning(f"Could not edit message for not useful feedback: {e}")
            await context.bot.send_message(chat_id=chat_id_to_reply, text="Ø´ÙƒØ±Ù‹Ø§ Ù„ØªÙ‚ÙŠÙŠÙ…Ùƒ. Ø³Ù†Ø£Ø®Ø°Ù‡ ÙÙŠ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±. ğŸ‘")
    else:
        logger.warning(f"Received unknown callback_data: '{callback_data}'")
        await context.bot.send_message(chat_id=chat_id_to_reply, text=f"ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø¶ØºØ·Ø© Ø²Ø± ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ© Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {callback_data}")


async def unknown_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await context.bot.send_message(chat_id=update.effective_chat.id, text="Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø±.")

# --- Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª ---
if __name__ == '__main__':
    logger.info("Starting bot with GDrive (CSV read), QR, and OpenAI features...")

    if not BOT_TOKEN:
        logger.critical("CRITICAL: BOT_TOKEN is not set. Bot cannot start.")
        exit()

    application = ApplicationBuilder().token(BOT_TOKEN).build()

    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª
    application.add_handler(CommandHandler('start', start_command))
    application.add_handler(CommandHandler('admin_test', admin_test_command))
    application.add_handler(CommandHandler('qr', qr_command_handler))
    application.add_handler(CommandHandler('testai', testai_command))
    application.add_handler(CommandHandler('askdata', askdata_command)) # Added /askdata handler
    application.add_handler(CommandHandler('gdrivefiles', list_gdrive_files_command))

    # --- !!! ØªØ³Ø¬ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬ CallbackQueryHandler Ø§Ù„ÙˆØ¸ÙŠÙÙŠ !!! ---
    # Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø³ÙŠÙ„ØªÙ‚Ø· ÙƒÙ„ Ø¶ØºØ·Ø§Øª Ø§Ù„Ø£Ø²Ø±Ø§Ø±ØŒ ÙˆØ§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ Ø³ÙŠÙˆØ¬Ù‡Ù‡Ø§
    application.add_handler(CallbackQueryHandler(button_callback))
    # Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø£ÙƒØ«Ø± ØªØ­Ø¯ÙŠØ¯Ù‹Ø§ Ù„Ø§Ø­Ù‚Ù‹Ø§ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… "pattern"
    # application.add_handler(CallbackQueryHandler(handle_read_csv_button, pattern=r'^read_csv_'))
    # application.add_handler(CallbackQueryHandler(handle_feedback_button, pattern=r'^feedback_'))

    # --- PDF Handler ---
    # This should be placed before the generic echo_message and unknown_command handlers
    # to ensure it catches PDF files specifically.
    pdf_file_handler = MessageHandler(filters.Document.FileExtension("pdf"), handle_pdf_document)
    application.add_handler(pdf_file_handler)

    # --- Generic Document Handler (for all other document types) ---
    # This should be after specific document handlers like PDF, but before generic message handlers.
    doc_upload_handler = MessageHandler(filters.Document.ALL, handle_document_upload)
    application.add_handler(doc_upload_handler)

    # --- Handlers for main keyboard button presses ---
    # These should be placed before the generic echo_message handler.
    application.add_handler(MessageHandler(filters.Regex(r"^ğŸ—‚ï¸ Ù…Ù„ÙØ§ØªÙŠ ÙÙŠ Drive$"), list_gdrive_files_command))
    application.add_handler(MessageHandler(filters.Regex(r"^â“ Ø§Ø³Ø£Ù„ Ø¹Ù† Ø¨ÙŠØ§Ù†Ø§ØªÙŠ$"), askdata_command))
    application.add_handler(MessageHandler(filters.Text(["ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© PDF"]), prompt_pdf_upload))
    application.add_handler(MessageHandler(filters.Text(["ğŸ“¤ Ø±ÙØ¹ Ù…Ù„Ù Ø¥Ù„Ù‰ Drive"]), prompt_general_upload))

    # --- Add Search Conversation Handler ---
    search_conv_handler = ConversationHandler(
        entry_points=[CommandHandler('searchdata', start_search_conversation)],
        states={
            SELECT_FILE: [MessageHandler(filters.TEXT & ~filters.COMMAND, received_filename_for_search)],
            SELECT_COLUMN: [CallbackQueryHandler(received_column_for_search, pattern=r'^search_col_select_\d+$')],
            SELECT_MATCH_TYPE: [CallbackQueryHandler(received_match_type, pattern=r'^searchmatch_')], # Pattern example
            INPUT_SEARCH_VALUE: [MessageHandler(filters.TEXT & ~filters.COMMAND, received_search_value)],
        },
        fallbacks=[CommandHandler('cancel_search', cancel_search_conversation)],
        # persistent=False, # Using default (memory-based persistence for now)
        # allow_reentry=True # Default is False, which is usually fine
    )
    application.add_handler(search_conv_handler)

    application.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), echo_message))
    application.add_handler(MessageHandler(filters.COMMAND, unknown_command)) # Keep this last for unhandled commands
    
    logger.info("Bot is now polling for updates...")
    application.run_polling()

    logger.info("Bot has stopped.")


# --- Search Results Handling ---
async def handle_search_results(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_id = update.effective_user.id
    search_params = context.user_data

    file_id = search_params.get('search_file_id')
    mime_type = search_params.get('search_mime_type')
    column_name = search_params.get('search_selected_column_name')
    match_type = search_params.get('search_match_type')
    search_value = search_params.get('search_value')
    file_name = search_params.get('search_file_name', 'Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø¯Ø¯')

    if not all([file_id, mime_type, column_name, match_type, search_value is not None]): # search_value can be empty string
        logger.error(f"User {user_id}: Missing search parameters in handle_search_results. Data: {search_params}")
        await update.message.reply_text("Ø­Ø¯Ø« Ø®Ø·Ø£ØŒ Ø¨Ø¹Ø¶ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ù…ÙÙ‚ÙˆØ¯Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
        return

    await update.message.reply_text(
        f"Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ù„Ù '{escape_markdown_v2(file_name)}' Ø¹Ù† Ù‚ÙŠÙ…Ø© '{escape_markdown_v2(search_value)}' ÙÙŠ Ø¹Ù…ÙˆØ¯ '{escape_markdown_v2(column_name)}' Ø¨Ù†ÙˆØ¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© '{match_type}'...",
        parse_mode='MarkdownV2'
    )
    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=ChatAction.TYPING)

    service = await get_gdrive_service_async()
    if not service:
        await update.message.reply_text("ØªØ¹Ø°Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Google Drive Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«.")
        return

    results_df_or_error_str = await perform_actual_search_async(
        service, file_id, mime_type, column_name, match_type, search_value
    )

    if isinstance(results_df_or_error_str, pd.DataFrame):
        results_df = results_df_or_error_str
        logger.info(f"User {user_id}: Search successful. Found {len(results_df)} results in '{file_name}'.")
        # The line below was commented out in the previous correct diff, so keeping it commented.
        # context.user_data['search_results_df'] = results_df

        if results_df.empty:
            await update.message.reply_text("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.")
        else:
            num_total_results = len(results_df)
            max_preview_rows = 10
            preview_df = results_df.head(max_preview_rows)

            try:
                preview_text = preview_df.to_string(index=False, na_rep='-')
            except Exception as e_to_str:
                logger.error(f"Error converting DataFrame to string for preview: {e_to_str}")
                preview_text = "Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø³ÙŠÙ‚ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬."

            max_chars_telegram = 4096
            message_header = f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {num_total_results} Ù†ØªÙŠØ¬Ø©."
            if num_total_results > 0:
                message_header += f"\nØ¥Ù„ÙŠÙƒ Ù…Ø¹Ø§ÙŠÙ†Ø© Ù„Ø£ÙˆÙ„ {min(num_total_results, max_preview_rows)} Ù†ØªÙŠØ¬Ø©:\n\n"

            code_block_ticks = "\n```\n"

            # Adjusted safety margin for more reliable truncation
            available_chars_for_text = max_chars_telegram - (len(message_header) + len(code_block_ticks) * 2 + 100)

            if len(preview_text) > available_chars_for_text:
                # Ensure we don't cut in the middle of a multi-byte character if string is unicode
                # However, Python string slicing is based on character counts, not bytes.
                # The main concern is the overall message length for Telegram.
                preview_text = preview_text[:available_chars_for_text] + "\n... (ØªÙ… Ø§Ù‚ØªØ·Ø§Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© Ù„Ø·ÙˆÙ„Ù‡Ø§ Ø§Ù„Ø²Ø§Ø¦Ø¯)"

            message = f"{message_header}```\n{preview_text}\n```"

            if num_total_results > max_preview_rows:
                message += f"\n\n(ÙŠØªÙ… Ø¹Ø±Ø¶ Ø£ÙˆÙ„ {max_preview_rows} Ù†ØªÙŠØ¬Ø© ÙÙ‚Ø· Ù…Ù† Ø¥Ø¬Ù…Ø§Ù„ÙŠ {num_total_results})."
                # TODO: Implement "Download full results as CSV" button here later
                await update.message.reply_text(message, parse_mode='MarkdownV2')
            else:
                await update.message.reply_text(message, parse_mode='MarkdownV2')

    else:
        error_str = results_df_or_error_str
        logger.error(f"User {user_id}: Search failed for file '{file_name}': {error_str}")
        await update.message.reply_text(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {escape_markdown_v2(error_str)}")


# --- /searchdata Conversation Functions ---
async def start_search_conversation(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    context.user_data.clear() # Clear any previous user_data from this conversation
    await update.message.reply_text(
        "Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…!\n"
        "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ù…Ù„Ù CSV Ø£Ùˆ Excel Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø§Ù„Ø¨Ø­Ø« ÙÙŠÙ‡ (Ø£Ùˆ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ø³Ù…Ù‡)."
    )
    return SELECT_FILE

async def received_filename_for_search(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    filename_query = update.message.text.strip()
    if not filename_query:
        await update.message.reply_text("Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ù…Ù„Ù. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ù…Ù„Ù ØµØ§Ù„Ø­ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… /cancel_search Ù„Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¨Ø­Ø«.")
        return SELECT_FILE

    await update.message.reply_text(f"Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„Ù Ø¨Ø§Ø³Ù…: '{escape_markdown_v2(filename_query)}'...")

    service = await get_gdrive_service_async()
    if not service:
        await update.message.reply_text("ØªØ¹Ø°Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Google Drive. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø¨Ø­Ø«. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… /cancel_search.")
        return ConversationHandler.END # Or SELECT_FILE to allow retry without full cancel

    found_file_id, found_file_name, found_mime_type = await find_file_in_gdrive_async(service, filename_query)

    compatible_mime_types = ['text/csv', 'application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']

    if found_file_id and found_mime_type in compatible_mime_types:
        context.user_data['search_file_id'] = found_file_id
        context.user_data['search_file_name'] = found_file_name
        context.user_data['search_mime_type'] = found_mime_type

        await update.message.reply_text(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù: '{escape_markdown_v2(found_file_name)}' (Ù†ÙˆØ¹: {found_mime_type}).\nØ¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©...")

        columns = await get_schema_from_file_async(service, found_file_id, found_mime_type)

        if columns: # Check if columns is not None and not empty
            context.user_data['search_file_columns'] = columns
            keyboard = [[InlineKeyboardButton(col, callback_data=f"search_col_select_{idx}")] for idx, col in enumerate(columns)]
            reply_markup = InlineKeyboardMarkup(keyboard)
            await update.message.reply_text("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø§Ù„Ø¨Ø­Ø« ÙÙŠÙ‡:", reply_markup=reply_markup)
            return SELECT_COLUMN
        else:
            await update.message.reply_text(
                f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„Ù '{escape_markdown_v2(found_file_name)}' ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©. "
                "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù ØºÙŠØ± ÙØ§Ø±ØºØŒ ÙˆØºÙŠØ± Ù…Ø­Ù…ÙŠ Ø¨ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ±ØŒ ÙˆÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØµÙ ØªØ±ÙˆÙŠØ³Ø© ØµØ­ÙŠØ­. "
                "Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø§Ø³Ù… Ù…Ù„Ù Ø¢Ø®Ø± Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… /cancel_search."
            )
            return SELECT_FILE # Stay in the same state to allow user to try another filename
    else:
        if found_file_id: # File found but not compatible type
             await update.message.reply_text(
                f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù '{escape_markdown_v2(found_file_name)}' ÙˆÙ„ÙƒÙ† Ù†ÙˆØ¹Ù‡ ({found_mime_type}) Ù„ÙŠØ³ CSV Ø£Ùˆ Excel. "
                "Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù†ÙˆØ¹ Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø§Ù„ÙŠÙ‹Ø§. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ù…Ù„Ù CSV Ø£Ùˆ Excel ØµØ§Ù„Ø­ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… /cancel_search."
            )
        else: # File not found
            await update.message.reply_text(
                "Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù CSV Ø£Ùˆ Excel Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³Ù…. "
                "Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø§Ø³Ù… ÙˆØ§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ØŒ Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… /cancel_search."
            )
        return SELECT_FILE


async def received_column_for_search(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    # Placeholder - Full implementation in next subtask
    query = update.callback_query
    await query.answer()
    selected_col_index = int(query.data.split('_')[-1])
    selected_column_name = context.user_data['search_file_columns'][selected_col_index]
    context.user_data['search_selected_column_name'] = selected_column_name
    context.user_data['search_selected_column_index'] = selected_col_index

    logger.info(f"User selected column '{selected_column_name}' (index {selected_col_index}) for file '{context.user_data.get('search_file_name')}'.")

    await query.edit_message_text(text=f"ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø¹Ù…ÙˆØ¯: '{escape_markdown_v2(selected_column_name)}'.\nØ§Ù„Ø¢Ù†ØŒ ÙƒÙŠÙ ØªØ±ÙŠØ¯ Ø§Ù„Ø¨Ø­Ø«ØŸ (Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©)")
    # For now, ending the conversation here. Next step will define SELECT_MATCH_TYPE state.
    # return ConversationHandler.END # Original placeholder end

    match_types = {
        'contains': "ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ (Ù†Øµ)",
        'exact': "ÙŠØ³Ø§ÙˆÙŠ ØªÙ…Ø§Ù…Ù‹Ø§ (Ù†Øµ/Ø±Ù‚Ù…)", # Changed 'equals' to 'exact' for clarity
        'not_contains': "Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ (Ù†Øµ)",
        'not_exact': "Ù„Ø§ ÙŠØ³Ø§ÙˆÙŠ ØªÙ…Ø§Ù…Ù‹Ø§ (Ù†Øµ/Ø±Ù‚Ù…)", # Changed 'not_equals'
        'greater_than': "Ø£ÙƒØ¨Ø± Ù…Ù† (Ø±Ù‚Ù…)",
        'less_than': "Ø£ØµØºØ± Ù…Ù† (Ø±Ù‚Ù…)",
        'starts_with': "ÙŠØ¨Ø¯Ø£ Ø¨Ù€ (Ù†Øµ)",
        'ends_with': "ÙŠÙ†ØªÙ‡ÙŠ Ø¨Ù€ (Ù†Øµ)"
    }
    keyboard = []
    # Create two buttons per row for a cleaner look
    row_buttons = []
    for key, text_label in match_types.items():
        row_buttons.append(InlineKeyboardButton(text_label, callback_data=f"search_match_type_{key}"))
        if len(row_buttons) == 2:
            keyboard.append(row_buttons)
            row_buttons = []
    if row_buttons: # Add any remaining button
        keyboard.append(row_buttons)

    reply_markup = InlineKeyboardMarkup(keyboard)

    await query.edit_message_text(
        text=f"ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø¹Ù…ÙˆØ¯: '{escape_markdown_v2(selected_column_name)}'.\nØ§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØªÙŠ Ø³ØªØ¨Ø­Ø« Ø¹Ù†Ù‡Ø§:",
        reply_markup=reply_markup,
        parse_mode='MarkdownV2'
    )
    return SELECT_MATCH_TYPE

async def received_match_type(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    # Placeholder - Full implementation in a future subtask
    query = update.callback_query
    await query.answer()
    # Storing the selected match type
    match_type_key = query.data.split('_')[-1]
    context.user_data['search_match_type'] = match_type_key

    # For user-friendliness, get the display text of the match type
    # This requires match_types to be accessible here or passed/redefined.
    # For now, just use the key.
    logger.info(f"User selected match type: {match_type_key}")

    await query.edit_message_text(text=f"ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©: {match_type_key}.\nØ§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†Ù‡Ø§ ÙÙŠ Ø¹Ù…ÙˆØ¯ '{escape_markdown_v2(context.user_data.get('search_selected_column_name', 'Ø§Ù„Ù…Ø­Ø¯Ø¯'))}'.")
    return INPUT_SEARCH_VALUE # Transition to inputting search value

async def received_search_value(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    search_value = update.message.text.strip()
    if not search_value:
        await update.message.reply_text("Ù„Ù… ÙŠØªÙ… Ø¥Ø¯Ø®Ø§Ù„ Ù‚ÙŠÙ…Ø© Ù„Ù„Ø¨Ø­Ø«. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù‚ÙŠÙ…Ø© Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… /cancel_search Ù„Ù„Ø¥Ù„ØºØ§Ø¡.")
        return INPUT_SEARCH_VALUE # Stay in the same state to prompt again

    context.user_data['search_value'] = search_value

    logger.info(f"Search parameters collected for user {update.effective_user.id}:")
    logger.info(f"  File ID: {context.user_data.get('search_file_id')}")
    logger.info(f"  File Name: {context.user_data.get('search_file_name')}")
    logger.info(f"  Selected Column Index: {context.user_data.get('search_selected_column_index')}")
    logger.info(f"  Selected Column Name: {context.user_data.get('search_selected_column_name')}")
    logger.info(f"  Match Type: {context.user_data.get('search_match_type')}")
    logger.info(f"  Search Value: {search_value}")

    # Placeholder for actual search logic
    await update.message.reply_text(
        f"ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… ÙƒÙ„ Ù…Ø¹Ø·ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø«.\n"
        f"Ø§Ù„Ù…Ù„Ù: `{escape_markdown_v2(context.user_data.get('search_file_name', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'))}`\n"
        f"Ø§Ù„Ø¹Ù…ÙˆØ¯: `{escape_markdown_v2(context.user_data.get('search_selected_column_name', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'))}`\n"
        f"Ù†ÙˆØ¹ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©: `{escape_markdown_v2(context.user_data.get('search_match_type', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'))}`\n"
        f"Ø§Ù„Ù‚ÙŠÙ…Ø© Ù„Ù„Ø¨Ø­Ø«: `{escape_markdown_v2(search_value)}`\n\n"
        "(Ù…Ù„Ø§Ø­Ø¸Ø©: Ø³ÙŠØªÙ… Ø§Ù„Ø¢Ù† ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ÙØ¹Ù„ÙŠ ÙˆØ¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ \\- Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙŠØ²Ø© Ù‚ÙŠØ¯ Ø§Ù„ØªØ·ÙˆÙŠØ± Ø­Ø§Ù„ÙŠÙ‹Ø§\\.)",
        parse_mode='MarkdownV2'
    )

    # TODO: In the next plan step, call a function here like:
    # await perform_actual_search(update, context) # This is now handle_search_results
    # For now, we just end the conversation after calling handle_search_results.

    await handle_search_results(update, context) # Call the new handler

    context.user_data.clear()
    return ConversationHandler.END

async def cancel_search_conversation(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    await update.message.reply_text("ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¨Ø­Ø«. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… /searchdata.")
    context.user_data.clear()
    return ConversationHandler.END